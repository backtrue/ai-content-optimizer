#!/usr/bin/env python3
"""
SERP è’é›†æ‰¹æ¬¡åŒ–æ¨¡çµ„
æ”¯æ´å¤–éƒ¨é—œéµå­—è¼¸å…¥ã€åˆ†æ‰¹åŸ·è¡Œã€R2 ä¸Šå‚³
"""

import os
import json
import csv
import time
import argparse
from datetime import datetime
from typing import List, Dict, Optional, Tuple
from pathlib import Path
import requests

# å°å…¥ç¾æœ‰çš„å·¥å…·
from serp_manager import get_manager as get_serp_manager
from cost_tracker import get_tracker as get_cost_tracker
from sheets_writer import get_sheets_writer, BASE_COLUMNS


class SerpCollectionBatch:
    """SERP è’é›†æ‰¹æ¬¡è™•ç†å™¨"""
    
    def __init__(
        self,
        keywords: List[str],
        output_dir: str = './ml',
        batch_size: int = 10,
        analyze_api_url: str = 'https://ragseo.thinkwithblack.com/api/analyze',
        keyword_delay: float = 15.0,
        url_delay: float = 12.0
    ):
        """
        åˆå§‹åŒ–æ‰¹æ¬¡è’é›†å™¨
        
        Args:
            keywords: é—œéµå­—æ¸…å–®
            output_dir: è¼¸å‡ºç›®éŒ„
            batch_size: æ¯æ‰¹è™•ç†çš„é—œéµå­—æ•¸
            analyze_api_url: åˆ†æ API URL
            keyword_delay: é—œéµå­—é–“éš”ï¼ˆç§’ï¼‰
            url_delay: URL é–“éš”ï¼ˆç§’ï¼‰
        """
        self.keywords = keywords
        self.output_dir = Path(output_dir)
        self.batch_size = batch_size
        self.analyze_api_url = analyze_api_url
        self.keyword_delay = keyword_delay
        self.url_delay = url_delay
        
        # åˆå§‹åŒ–å·¥å…·
        self.serp_manager = get_serp_manager()
        self.cost_tracker = get_cost_tracker()
        self.sheets_writer = get_sheets_writer()
        
        # è¼¸å‡ºç›®éŒ„
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # é€²åº¦è¿½è¹¤
        self.records = []
        self.processed_count = 0
        self.failed_count = 0
        self.start_time = None
    
    def load_keywords_from_file(self, filepath: str) -> List[str]:
        """å¾ JSON æˆ– CSV æª”æ¡ˆè¼‰å…¥é—œéµå­—"""
        path = Path(filepath)
        
        if not path.exists():
            raise FileNotFoundError(f"é—œéµå­—æª”æ¡ˆä¸å­˜åœ¨: {filepath}")
        
        keywords = []
        
        if path.suffix == '.json':
            with open(path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if isinstance(data, list):
                    keywords = [str(kw).strip() for kw in data if kw]
                elif isinstance(data, dict) and 'keywords' in data:
                    keywords = [str(kw).strip() for kw in data['keywords'] if kw]
        
        elif path.suffix == '.csv':
            with open(path, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    kw = row.get('keyword', '').strip()
                    if kw:
                        keywords.append(kw)
        
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼: {path.suffix}")
        
        print(f"âœ… å·²è¼‰å…¥ {len(keywords)} ç­†é—œéµå­—")
        return keywords
    
    def fetch_serp_results(self, keyword: str) -> List[Dict]:
        """å–å¾— SERP çµæœ"""
        try:
            results, error, service = self.serp_manager.fetch(keyword)
            
            service_name = (service or 'unknown').lower().replace('api', '').replace('serp', '').strip()
            self.cost_tracker.record_request(service_name, success=(results is not None))
            
            if results:
                print(f"  âœ“ å–å¾— {len(results)} ç­†çµæœ (ä¾†æº: {service})")
                return results
            else:
                print(f"  âœ— éŒ¯èª¤: {error}")
                return []
        
        except Exception as e:
            print(f"  âœ— SERP è’é›†å¤±æ•—: {e}")
            return []
    
    def analyze_url(self, url: str, keyword: str, rank: int) -> Dict:
        """åˆ†æå–®å€‹ URL"""
        if not url:
            return {
                'url': url,
                'keyword': keyword,
                'rank': rank,
                'analysis_status': 'failed',
                'analysis_error': 'Empty URL'
            }
        
        try:
            payload = {
                'contentUrl': url,
                'targetKeywords': [keyword],
                'returnChunks': False
            }
            
            response = requests.post(
                self.analyze_api_url,
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                analysis = response.json()
                return {
                    'url': url,
                    'keyword': keyword,
                    'rank': rank,
                    'analysis_status': 'success',
                    'analysis': analysis
                }
            else:
                return {
                    'url': url,
                    'keyword': keyword,
                    'rank': rank,
                    'analysis_status': 'failed',
                    'analysis_error': f'HTTP {response.status_code}'
                }
        
        except Exception as e:
            return {
                'url': url,
                'keyword': keyword,
                'rank': rank,
                'analysis_status': 'failed',
                'analysis_error': str(e)
            }
    
    def process_keyword(self, keyword: str, keyword_index: int, total_keywords: int) -> List[Dict]:
        """è™•ç†å–®å€‹é—œéµå­—"""
        print(f"\nğŸ“Œ [{keyword_index}/{total_keywords}] è™•ç†é—œéµå­—: {keyword}")
        
        keyword_records = []
        
        # å–å¾— SERP çµæœ
        serp_results = self.fetch_serp_results(keyword)
        if not serp_results:
            print(f"  âš ï¸ æœªå–å¾— SERP çµæœï¼Œè·³éæ­¤é—œéµå­—")
            return keyword_records
        
        # åˆ†ææ¯å€‹ URL
        for rank, result in enumerate(serp_results, 1):
            url = result.get('link', '')
            title = result.get('title', '')
            
            print(f"  åˆ†æ [{rank}/10] {url[:60]}...")
            
            # åˆ†æ URL
            analysis_result = self.analyze_url(url, keyword, rank)
            
            # çµ„åˆè¨˜éŒ„
            record = {
                'keyword': keyword,
                'url': url,
                'title': title,
                'rank': rank,
                'timestamp': datetime.now().isoformat(),
                **analysis_result
            }
            
            keyword_records.append(record)
            self.processed_count += 1
            
            # å»¶é²
            time.sleep(self.url_delay)
        
        # å»¶é²ï¼ˆé—œéµå­—é–“éš”ï¼‰
        time.sleep(self.keyword_delay)
        
        return keyword_records
    
    def process_batch(self, batch_keywords: List[str], batch_index: int, total_batches: int) -> List[Dict]:
        """è™•ç†ä¸€å€‹æ‰¹æ¬¡"""
        print(f"\nğŸ”„ æ‰¹æ¬¡ [{batch_index}/{total_batches}] é–‹å§‹")
        
        batch_records = []
        
        for i, keyword in enumerate(batch_keywords, 1):
            keyword_index = (batch_index - 1) * self.batch_size + i
            records = self.process_keyword(keyword, keyword_index, len(self.keywords))
            batch_records.extend(records)
            
            # å®šæœŸä¿å­˜é€²åº¦
            if i % 5 == 0:
                self.save_progress(batch_records)
        
        print(f"âœ… æ‰¹æ¬¡ [{batch_index}/{total_batches}] å®Œæˆ")
        return batch_records
    
    def save_progress(self, batch_records: List[Dict]) -> None:
        """ä¿å­˜é€²åº¦"""
        self.records.extend(batch_records)
        
        # ä¿å­˜ JSON
        json_path = self.output_dir / f"serp_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(self.records, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜ CSV
        csv_path = self.output_dir / f"serp_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        self.save_csv(csv_path, self.records)
        
        print(f"  ğŸ’¾ å·²ä¿å­˜é€²åº¦: {len(self.records)} ç­†è¨˜éŒ„")
    
    def save_csv(self, filepath: Path, records: List[Dict]) -> None:
        """ä¿å­˜ç‚º CSV"""
        if not records:
            return
        
        # æå–æ‰€æœ‰æ¬„ä½
        fieldnames = set()
        for record in records:
            fieldnames.update(record.keys())
        
        fieldnames = sorted(list(fieldnames))
        
        with open(filepath, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            for record in records:
                writer.writerow(record)
    
    def run(self) -> Dict:
        """åŸ·è¡Œæ‰¹æ¬¡è’é›†"""
        print(f"ğŸš€ é–‹å§‹ SERP è’é›†æ‰¹æ¬¡åŒ–")
        print(f"  é—œéµå­—æ•¸: {len(self.keywords)}")
        print(f"  æ‰¹æ¬¡å¤§å°: {self.batch_size}")
        print(f"  ç¸½æ‰¹æ¬¡æ•¸: {(len(self.keywords) + self.batch_size - 1) // self.batch_size}")
        
        self.start_time = datetime.now()
        
        # åˆ†æ‰¹è™•ç†
        total_batches = (len(self.keywords) + self.batch_size - 1) // self.batch_size
        
        for batch_index in range(total_batches):
            start_idx = batch_index * self.batch_size
            end_idx = min(start_idx + self.batch_size, len(self.keywords))
            batch_keywords = self.keywords[start_idx:end_idx]
            
            try:
                batch_records = self.process_batch(batch_keywords, batch_index + 1, total_batches)
                self.save_progress(batch_records)
            except Exception as e:
                print(f"âŒ æ‰¹æ¬¡ {batch_index + 1} å¤±æ•—: {e}")
                self.failed_count += 1
        
        # æœ€çµ‚çµ±è¨ˆ
        elapsed = datetime.now() - self.start_time
        
        summary = {
            'status': 'completed',
            'total_keywords': len(self.keywords),
            'total_records': len(self.records),
            'processed_count': self.processed_count,
            'failed_count': self.failed_count,
            'elapsed_seconds': elapsed.total_seconds(),
            'output_dir': str(self.output_dir),
            'completed_at': datetime.now().isoformat()
        }
        
        print(f"\nâœ… è’é›†å®Œæˆ")
        print(f"  ç¸½è€—æ™‚: {elapsed}")
        print(f"  è¨˜éŒ„æ•¸: {len(self.records)}")
        print(f"  æˆåŠŸ: {self.processed_count}")
        print(f"  å¤±æ•—: {self.failed_count}")
        
        return summary


def main():
    """ä¸»ç¨‹å¼"""
    parser = argparse.ArgumentParser(
        description='SERP è’é›†æ‰¹æ¬¡åŒ–è™•ç†'
    )
    parser.add_argument(
        '--keywords-file',
        help='é—œéµå­—æª”æ¡ˆè·¯å¾‘ (JSON æˆ– CSV)'
    )
    parser.add_argument(
        '--keywords-json',
        help='é—œéµå­— JSON å­—ä¸²'
    )
    parser.add_argument(
        '--output-dir',
        default='./ml',
        help='è¼¸å‡ºç›®éŒ„'
    )
    parser.add_argument(
        '--batch-size',
        type=int,
        default=10,
        help='æ¯æ‰¹é—œéµå­—æ•¸'
    )
    parser.add_argument(
        '--analyze-api-url',
        default='https://ragseo.thinkwithblack.com/api/analyze',
        help='åˆ†æ API URL'
    )
    parser.add_argument(
        '--keyword-delay',
        type=float,
        default=15.0,
        help='é—œéµå­—é–“éš”ï¼ˆç§’ï¼‰'
    )
    parser.add_argument(
        '--url-delay',
        type=float,
        default=12.0,
        help='URL é–“éš”ï¼ˆç§’ï¼‰'
    )
    
    args = parser.parse_args()
    
    # è¼‰å…¥é—œéµå­—
    keywords = []
    
    if args.keywords_file:
        collector = SerpCollectionBatch([], args.output_dir)
        keywords = collector.load_keywords_from_file(args.keywords_file)
    
    elif args.keywords_json:
        try:
            keywords = json.loads(args.keywords_json)
            if not isinstance(keywords, list):
                raise ValueError('keywords_json å¿…é ˆæ˜¯é™£åˆ—')
            print(f"âœ… å·²è¼‰å…¥ {len(keywords)} ç­†é—œéµå­—")
        except json.JSONDecodeError as e:
            print(f"âŒ é—œéµå­— JSON è§£æå¤±æ•—: {e}")
            return
    
    else:
        print("âŒ å¿…é ˆæä¾› --keywords-file æˆ– --keywords-json")
        parser.print_help()
        return
    
    if not keywords:
        print("âŒ æœªæä¾›ä»»ä½•é—œéµå­—")
        return
    
    # åŸ·è¡Œè’é›†
    collector = SerpCollectionBatch(
        keywords=keywords,
        output_dir=args.output_dir,
        batch_size=args.batch_size,
        analyze_api_url=args.analyze_api_url,
        keyword_delay=args.keyword_delay,
        url_delay=args.url_delay
    )
    
    summary = collector.run()
    
    # ä¿å­˜æ‘˜è¦
    summary_path = Path(args.output_dir) / f"batch_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(summary_path, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“Š æ‘˜è¦å·²ä¿å­˜: {summary_path}")


if __name__ == '__main__':
    main()
