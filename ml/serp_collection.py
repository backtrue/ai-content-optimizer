#!/usr/bin/env python3
"""
SERP Data Collection Script
Fetches top 10 results from ValueSerp for given keywords and extracts features via our /analyze API.
"""

import os
import json
import csv
import time
import requests
from collections import Counter
from typing import List, Dict, Optional, Tuple, Any
from datetime import datetime
from serp_manager import get_manager as get_serp_manager
from cost_tracker import get_tracker as get_cost_tracker
from sheets_writer import get_sheets_writer, BASE_COLUMNS


def load_env_variables() -> Optional[str]:
    """ÂòóË©¶ËºâÂÖ•Êú¨Âú∞Áí∞Â¢ÉÊ™îÔºåËÆìËÖ≥Êú¨ÂèØÁõ¥Êé•‰ΩøÁî® .env Ë®≠ÂÆö„ÄÇ"""
    candidate_files = ['.env.serp', '.env', '.env.serp.local', '.env.serp.example']

    def _parse_and_set(path: str) -> None:
        with open(path, 'r', encoding='utf-8') as handle:
            for raw_line in handle:
                line = raw_line.strip()
                if not line or line.startswith('#'):
                    continue
                if '=' not in line:
                    continue
                key, value = line.split('=', 1)
                key = key.strip()
                value = value.strip()
                if value and len(value) >= 2 and value[0] == value[-1] and value[0] in ('"', "'"):
                    value = value[1:-1]
                os.environ[key] = value

    for filename in candidate_files:
        if os.path.exists(filename):
            _parse_and_set(filename)
            print(f"‚öôÔ∏è Â∑≤ËºâÂÖ•Áí∞Â¢ÉË®≠ÂÆöÊ™îÔºö{filename}")
            return filename

    return None



load_env_variables()


def is_sheets_sync_enabled() -> bool:
    """Ê†πÊìöÁí∞Â¢ÉËÆäÊï∏Ê±∫ÂÆöÊòØÂê¶ÂïüÁî® Google Sheets ÂêåÊ≠•„ÄÇ"""
    flag = os.getenv("SERP_TASK_SYNC_TO_SHEETS")
    if flag is None:
        flag = os.getenv("SERP_SYNC_TO_SHEETS", "true")
    return flag.lower() in {"1", "true", "yes", "on"}

# Configuration
SERPAPI_KEY = os.getenv('SERPAPI_KEY', '')
ANALYZE_API_URL = os.getenv('ANALYZE_API_URL', 'https://ragseo.thinkwithblack.com/api/analyze')
OUTPUT_DIR = './ml'
OUTPUT_CSV = os.path.join(OUTPUT_DIR, 'training_data.csv')
OUTPUT_JSON = os.path.join(OUTPUT_DIR, 'training_data.json')
STATUS_JSON = os.path.join(OUTPUT_DIR, 'collection_status.json')

# Rate limiting & retry configuration
KEYWORD_DELAY_SECONDS = float(os.getenv('SERP_KEYWORD_DELAY_SECONDS', '15'))
URL_DELAY_SECONDS = float(os.getenv('SERP_URL_DELAY_SECONDS', '12'))
MIN_RESULTS_PER_KEYWORD = int(os.getenv('SERP_MIN_RESULTS_PER_KEYWORD', '10'))
ANALYZE_RETRY_ATTEMPTS = int(os.getenv('SERP_ANALYZE_RETRIES', '3'))
ANALYZE_RETRY_DELAY_SECONDS = float(os.getenv('SERP_ANALYZE_RETRY_DELAY_SECONDS', '20'))

# Keywords to analyze (ÂèØÁî±Áí∞Â¢ÉËÆäÊï∏Ë¶ÜÂØ´)
def _load_keywords() -> List[str]:
    env_keywords = os.getenv('SERP_KEYWORDS_JSON')
    if env_keywords:
        try:
            data = json.loads(env_keywords)
            if isinstance(data, list):
                return [str(item) for item in data if isinstance(item, str) and item.strip()]
        except json.JSONDecodeError:
            print('‚ö†Ô∏è SERP_KEYWORDS_JSON Ëß£ÊûêÂ§±ÊïóÔºåÊîπÁî®È†êË®≠ÈóúÈçµÂ≠óÊ∏ÖÂñÆ')
    return [
        "iPhone 17 Pro Max", "momo Ë≥ºÁâ©Á∂≤", "Ëù¶ÁöÆ", "ËóçËäΩËÄ≥Ê©ü Êé®Ëñ¶", "Coupang ÈÖ∑Êæé",
        "Èô§ÊøïÊ©ü Êé®Ëñ¶", "Costco ÂøÖË≤∑", "Ê∞£ÁÇ∏Èçã Êé®Ëñ¶", "Á≠ÜÈõª Êé®Ëñ¶ 2025", "ÂÖ®ËÅØ ÂøÖË≤∑",
        "Dyson Âê∏Â°µÂô®", "AirPods Pro 3", "PS5 Pro", "Switch ÈÅäÊà≤ Êé®Ëñ¶", "iHerb Êé®Ëñ¶",
        "Uniqlo", "Nike ÂÆòÁ∂≤", "Adidas Samba", "Lululemon", "ËÅñË™ï ‰∫§Êèõ Á¶ÆÁâ© 500ÂÖÉ",
        "ÊâãÊ©ü Êé®Ëñ¶ PTT", "‰øùÊøï Á≤æËèØÊ∂≤ Êé®Ëñ¶", "Ë≤ìÁ†Ç Êé®Ëñ¶", "PChome 24h", "ÂçöÂÆ¢‰æÜ",
        "ÁîüÊó• Á¶ÆÁâ© Êé®Ëñ¶", "SOGO ÈÄ±Âπ¥ÊÖ∂", "Êñ∞ÂÖâ‰∏âË∂ä ÈÄ±Âπ¥ÊÖ∂", "ÈÅéÂπ¥ Á¶ÆÁõí Êé®Ëñ¶", "Èú≤Ááü Áî®ÂìÅ Ê∏ÖÂñÆ",
        "Sony WH-1000XM6", "Samsung S25 Ultra", "ÁÑ°Âç∞ËâØÂìÅ ÂøÖË≤∑", "ÁâπÊñØÊãâ Model Y Ë©ïÂÉπ", "Gogoro Ë©ïÂÉπ",
        "Ë°õÁîüÁ¥ô ÁâπÂÉπ", "ÊòüÂ∑¥ÂÖã Ë≤∑‰∏ÄÈÄÅ‰∏Ä", "È∫•Áï∂Âãû ÂÑ™ÊÉ†Âà∏", "Uber Eats ÂÑ™ÊÉ†Á¢º", "Foodpanda ÂÖçÈÅã",
        "ÈõªÁ´∂Ê§Ö Êé®Ëñ¶", "RTX 5080 ÂÉπÊ†º", "Macbook M4 Ë©ïÂÉπ", "Ë°åÂãïÈõªÊ∫ê Êé®Ëñ¶", "Á≠ãËÜúÊßç Êé®Ëñ¶",
        "Âè∞Âåó È§êÂª≥ Êé®Ëñ¶", "Ê¥óÈ´ÆÁ≤æ Êé®Ëñ¶ Dcard", "Á©∫Ê∞£Ê∏ÖÊ∑®Ê©ü Êé®Ëñ¶", "ËÜ†ÂéüËõãÁôΩ Êé®Ëñ¶", "ÁõäÁîüËèå Êé®Ëñ¶",
        "SK-II ÈùíÊò•Èú≤", "Jo Malone", "ÂØ∂ÂØ∂ Â•∂Á≤â Êé®Ëñ¶", "Â∞øÂ∏É S", "ÊØçË¶™ÁØÄ Á¶ÆÁâ© ÊéíË°å",
        "Áà∂Ë¶™ÁØÄ Á¶ÆÁâ©", "ÊÉÖ‰∫∫ÁØÄ Á¶ÆÁâ©", "iPhone 17 ÂÉπÊ†º", "‰æøÂÆú Ê©üÁ•®", "Stanley ‰øùÊ∫´ÊùØ",
        "Nespresso ËÜ†Âõä", "ÂØµÁâ© È£≤Ê∞¥Ê©ü", "Dyson ÂêπÈ¢®Ê©ü", "iPhone 17 vs Pixel 10", "Nintendo Switch 2",
        "Èõô 11 ÂÑ™ÊÉ†", "Black Friday ÂÑ™ÊÉ†", "ÂúãÊ≥∞ CUBE Âç° ÂÑ™ÊÉ†", "momo ÊäòÂÉπÂà∏", "Ëù¶ÁöÆ ÂÖçÈÅã",
        "Ëæ¶ÂÖ¨ÂÆ§ ÂúòË≥º Èõ∂È£ü", "Ê∞¥È§É ÂúòË≥º", "ÁëúÁèàÂ¢ä Êé®Ëñ¶", "Patagonia", "The North Face Â§ñÂ•ó",
        "Ê®ÇÈ´ò Êé®Ëñ¶", "Netflix Êé®Ëñ¶", "ÈõªÂΩ±Á•® ÂÑ™ÊÉ†", "Apple Watch 11", "ÂΩåÊúà Á¶ÆÁõí",
        "Êê¨ÂÆ∂ Á¶ÆÁâ©", "Rimowa", "Ê±ΩËªä ÈöîÁÜ±Á¥ô Êé®Ëñ¶", "ÁæéÂÆπÂÑÄ Êé®Ëñ¶", "Ê∞∏Á∫å ÂìÅÁâå",
        "AI Ë™≤Á®ã", "ÁáïÁ™© Êé®Ëñ¶", "Êª¥ÈõûÁ≤æ Êé®Ëñ¶", "ÊûóËÅ∞Êòé Á†ÇÈçãÈ≠öÈ†≠ Ê≥°È∫µ", "ÈùíËä±È©ï È∫ªËæ£ÁâõËÇâÈ∫µ",
        "ÊºîÂî±ÊúÉ ÈñÄÁ•®", "Ë™†ÂìÅ Á∑ö‰∏ä", "Ê∑òÂØ∂", "SHEIN", "Ëñ©ÁàæÈÅîÂÇ≥Ë™™",
        "ÂÖßË°£ Êé®Ëñ¶", "Ë∑ëÈûã Êé®Ëñ¶", "ÂàÆÈ¨çÂàÄ Êé®Ëñ¶", "IKEA ÂøÖË≤∑", "ÊøæÊéõ ÂíñÂï° Êé®Ëñ¶"
    ]


KEYWORDS = _load_keywords()


def load_existing_data() -> List[Dict]:
    """ËºâÂÖ•Êó¢ÊúâË®ìÁ∑¥Ë≥áÊñôÔºåËã•Êú¨Âú∞Ê™îÊ°à‰∏çÂ≠òÂú®ÂâáÂòóË©¶Âæû Google Sheets ËÆÄÂèñ„ÄÇ"""
    if os.path.exists(OUTPUT_JSON):
        try:
            with open(OUTPUT_JSON, 'r', encoding='utf-8') as file:
                data = json.load(file)
                if isinstance(data, list):
                    return data
                print("‚ö†Ô∏è training_data.json Ê†ºÂºèÈùûÈô£ÂàóÔºåÂ∞áÂøΩÁï•Êó¢ÊúâË≥áÊñô„ÄÇ")
        except json.JSONDecodeError:
            print("‚ö†Ô∏è training_data.json Ê≠£Âú®ÂØ´ÂÖ•ÊàñÊ†ºÂºè‰∏çÂÆåÊï¥ÔºåÊö´ÊôÇÂøΩÁï•Êó¢ÊúâË≥áÊñô„ÄÇ")
        except Exception as exc:
            print(f"‚ö†Ô∏è ËºâÂÖ•Êó¢ÊúâË≥áÊñôÊôÇÁôºÁîüÈåØË™§Ôºö{exc}")

    if not is_sheets_sync_enabled():
        return []

    sheets_writer = get_sheets_writer()
    if not sheets_writer:
        return []

    try:
        sheet_records = sheets_writer.fetch_all_records()
        records: List[Dict] = []
        for row in sheet_records:
            features = {k: v for k, v in row.items() if k not in BASE_COLUMNS}
            record = {
                'url': row.get('url', ''),
                'keyword': row.get('keyword', ''),
                'serp_rank': row.get('serp_rank', ''),
                'target_score': row.get('target_score', ''),
                'title': row.get('title', ''),
                'features': features
            }
            records.append(record)
        if records:
            print(f"üßæ Â∑≤Âæû Google Sheets ËÆÄÂÖ• {len(records)} Á≠ÜÊó¢ÊúâË≥áÊñô")
        return records
    except Exception as exc:  # pylint: disable=broad-except
        print(f"‚ö†Ô∏è Âæû Google Sheets ËÆÄÂèñË≥áÊñôÊôÇÂ§±ÊïóÔºö{exc}")
        return []


def save_csv(records: List[Dict]) -> None:
    """Ë¶ÜÂØ´Ëº∏Âá∫ CSVÔºå‰ª•‰æøÂç≥ÊôÇÊ™¢Ë¶ñÊúÄÊñ∞Ë≥áÊñô„ÄÇ"""
    if not records:
        if os.path.exists(OUTPUT_CSV):
            os.remove(OUTPUT_CSV)
        return

    feature_fields: List[str] = []
    for record in records:
        features = record.get('features', {})
        if not isinstance(features, dict):
            continue
        for key in features.keys():
            if key not in feature_fields:
                feature_fields.append(key)

    fieldnames = ['url', 'keyword', 'serp_rank', 'target_score', 'title'] + feature_fields
    with open(OUTPUT_CSV, 'w', newline='', encoding='utf-8') as csv_file:
        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)
        writer.writeheader()
        for record in records:
            row = {
                'url': record['url'],
                'keyword': record['keyword'],
                'serp_rank': record['serp_rank'],
                'target_score': record['target_score'],
                'title': record['title']
            }
            row.update(record.get('features', {}))
            writer.writerow(row)


def update_status_file(
    records: List[Dict],
    current_keyword: Optional[str],
    keyword_index: int,
    keyword_total: Optional[int] = None
) -> None:
    """Ëº∏Âá∫ËíêÈõÜÁãÄÊÖã JSONÔºåÊèê‰æõÁõ£ÊéßËÖ≥Êú¨Ëàá‰∫∫Â∑•Âø´ÈÄüÊ™¢Ë¶ñ„ÄÇ"""
    keyword_total = keyword_total if keyword_total is not None else len(KEYWORDS)
    unique_keywords = {item['keyword'] for item in records}
    status_payload = {
        'timestamp': datetime.now().isoformat(),
        'total_records': len(records),
        'unique_keywords': len(unique_keywords),
        'current_keyword': current_keyword,
        'current_keyword_index': keyword_index,
        'progress_percent': round(len(unique_keywords) / keyword_total * 100, 2) if keyword_total else 0,
        'remaining_keywords': max(0, keyword_total - len(unique_keywords)) if keyword_total else 0
    }

    with open(STATUS_JSON, 'w', encoding='utf-8') as status_file:
        json.dump(status_payload, status_file, ensure_ascii=False, indent=2)


def persist_progress(
    records: List[Dict],
    current_keyword: Optional[str],
    keyword_index: int,
    *,
    keyword_total: Optional[int] = None,
    persist_local: bool = True,
    update_status: bool = True,
    sync_to_sheets: bool = True,
    sheets_writer=None
) -> None:
    """Ê†πÊìöË®≠ÂÆöÂØ´ÂÖ•ÈÄ≤Â∫¶Ê™îÊ°à„ÄÅÁãÄÊÖãËàá Google Sheets„ÄÇ"""
    if persist_local:
        with open(OUTPUT_JSON, 'w', encoding='utf-8') as json_file:
            json.dump(records, json_file, ensure_ascii=False, indent=2)
        save_csv(records)

    if update_status:
        update_status_file(records, current_keyword, keyword_index, keyword_total)

    if not sync_to_sheets or current_keyword is None or not records:
        return

    # Sheets ÂØ´ÂÖ•Â∑≤Âú® sheets_writer.append_record ‰∏≠Âä†‰∏äÈåØË™§‰øùË≠∑
    writer = sheets_writer or get_sheets_writer()
    if not writer:
        return

    try:
        writer.append_record(records[-1])
        print("  ‚Üª Â∑≤ÂêåÊ≠•ÊúÄÊñ∞Á¥ÄÈåÑËá≥ Google Sheets")
    except Exception as exc:  # pylint: disable=broad-except
        print(f"‚ö†Ô∏è ÂØ´ÂÖ• Google Sheets Â§±ÊïóÔºö{exc}")


def record_signature(keyword: str, url: str) -> Tuple[str, str]:
    return keyword, url


# SERP rank to score mapping
def rank_to_score(rank: int) -> int:
    """Convert SERP rank (1-10) to quality score (0-100)."""
    rank_score_map = {
        1: 100,
        2: 95,
        3: 90,
        4: 85,
        5: 80,
        6: 75,
        7: 70,
        8: 65,
        9: 60,
        10: 55
    }
    return rank_score_map.get(rank, 50)


def fetch_serp_results(keyword: str) -> List[Dict]:
    """Fetch top 10 SERP results using multi-service manager with automatic failover."""
    manager = get_serp_manager()
    tracker = get_cost_tracker()
    
    print(f"  Fetching SERP for: {keyword}")
    results, error, service = manager.fetch(keyword)
    
    # Track API usage
    service_name = service.lower().replace('api', '').replace('serp', '').strip() if service else 'unknown'
    tracker.record_request(service_name, success=(results is not None))
    
    if results:
        print(f"    ‚úì Got {len(results)} results from {service}")
        return results
    else:
        print(f"    ‚úó Error: {error}")
        return []


def analyze_url(url: str, keyword: str) -> Dict:
    """Call our /analyze API to extract features for a URL, with retry support and failure metadata."""
    if not url:
        return {
            'analysis_status': 'failed',
            'analysis_error_kind': 'invalid_url',
            'analysis_error_message': 'Empty URL'
        }

    payload = {
        'contentUrl': url,
        'targetKeywords': [keyword],
        'returnChunks': False
    }

    last_error: Optional[Dict[str, Any]] = None

    for attempt in range(1, ANALYZE_RETRY_ATTEMPTS + 1):
        try:
            print(f"    Analyzing: {url[:60]}... (attempt {attempt}/{ANALYZE_RETRY_ATTEMPTS})")
            response = requests.post(ANALYZE_API_URL, json=payload, timeout=60)
            response.raise_for_status()
            data = response.json()

            # Extract features from response
            signals = data.get('contentSignals', {})
            flags = data.get('scoreGuards', {}).get('contentQualityFlags', {})
            hcu_counts = data.get('scoreGuards', {}).get('hcuCounts', {})

            # Ë®àÁÆó HCU Áõ∏ÈóúÁâπÂæµ
            total_hcu = hcu_counts.get('yes', 0) + hcu_counts.get('partial', 0) + hcu_counts.get('no', 0)
            hcu_yes_ratio = hcu_counts.get('yes', 0) / total_hcu if total_hcu > 0 else 0
            hcu_partial_ratio = hcu_counts.get('partial', 0) / total_hcu if total_hcu > 0 else 0
            hcu_no_ratio = hcu_counts.get('no', 0) / total_hcu if total_hcu > 0 else 0
            hcu_content_helpfulness = (hcu_yes_ratio + hcu_partial_ratio * 0.5) if total_hcu > 0 else 0

            # ÂÖßÂÆπÁµêÊßãÁâπÂæµ
            qa_format_score = min(1.0, signals.get('qaFormatScore', 0))
            first_para_answer_quality = min(1.0, signals.get('firstParagraphAnswerQuality', 0))
            semantic_paragraph_focus = min(1.0, signals.get('semanticParagraphFocus', 0))
            heading_hierarchy_quality = min(1.0, signals.get('headingHierarchyQuality', 0))
            topic_cohesion = min(1.0, signals.get('topicCohesion', 0))

            # ÊäÄË°ìËàáÊ®ôË®òÁâπÂæµ
            faq_schema_present = 1 if signals.get('faqSchemaPresent') else 0
            howto_schema_present = 1 if signals.get('howtoSchemaPresent') else 0
            article_schema_present = 1 if signals.get('articleSchemaPresent') else 0
            organization_schema_present = 1 if signals.get('organizationSchemaPresent') else 0
            og_tags_complete = min(1.0, signals.get('ogTagsComplete', 0))
            meta_tags_quality = min(1.0, signals.get('metaTagsQuality', 0))
            html_structure_validity = min(1.0, signals.get('htmlStructureValidity', 0))

            # ÂìÅÁâåÂØ¶È´îËàá‰ø°‰ªªÁâπÂæµ
            author_info_present = 1 if signals.get('authorInfoPresent') else 0
            brand_entity_clarity = min(1.0, signals.get('brandEntityClarity', 0))
            external_citation_count = min(1.0, signals.get('externalCitationCount', 0) / 10)
            social_media_links_present = 1 if signals.get('socialMediaLinksPresent') else 0
            review_rating_present = 1 if signals.get('reviewRatingPresent') else 0

            # AI ÊêúÂ∞ãÈÅ©ÈÖçÁâπÂæµ
            semantic_naturalness = min(1.0, signals.get('semanticNaturalness', 0))
            paragraph_extractability = min(1.0, signals.get('paragraphExtractability', 0))
            rich_snippet_format = min(1.0, signals.get('richSnippetFormat', 0))
            citability_trust_score = min(1.0, signals.get('citabilityTrustScore', 0))
            multimedia_support = min(1.0, signals.get('multimediaSupport', 0))

            # ‰øùÁïôÊó¢ÊúâÁâπÂæµÔºàÂêëÂæåÁõ∏ÂÆπÔºâ
            features = {
                # HCU ÁâπÂæµ
                'hcuYesRatio': hcu_yes_ratio,
                'hcuPartialRatio': hcu_partial_ratio,
                'hcuNoRatio': hcu_no_ratio,
                'hcuContentHelpfulness': hcu_content_helpfulness,

                # ÂÖßÂÆπÁµêÊßã
                'qaFormatScore': qa_format_score,
                'firstParagraphAnswerQuality': first_para_answer_quality,
                'semanticParagraphFocus': semantic_paragraph_focus,
                'headingHierarchyQuality': heading_hierarchy_quality,
                'topicCohesion': topic_cohesion,

                # ÊäÄË°ìËàáÊ®ôË®ò
                'faqSchemaPresent': faq_schema_present,
                'howtoSchemaPresent': howto_schema_present,
                'articleSchemaPresent': article_schema_present,
                'organizationSchemaPresent': organization_schema_present,
                'ogTagsComplete': og_tags_complete,
                'metaTagsQuality': meta_tags_quality,
                'htmlStructureValidity': html_structure_validity,

                # ÂìÅÁâåÂØ¶È´îËàá‰ø°‰ªª
                'authorInfoPresent': author_info_present,
                'brandEntityClarity': brand_entity_clarity,
                'externalCitationCount': external_citation_count,
                'socialMediaLinksPresent': social_media_links_present,
                'reviewRatingPresent': review_rating_present,

                # AI ÊêúÂ∞ãÈÅ©ÈÖç
                'semanticNaturalness': semantic_naturalness,
                'paragraphExtractability': paragraph_extractability,
                'richSnippetFormat': rich_snippet_format,
                'citabilityTrustScore': citability_trust_score,
                'multimediaSupport': multimedia_support,

                # ‰øùÁïôÊó¢ÊúâÁâπÂæµÔºàÂêëÂæåÁõ∏ÂÆπÔºâ
                'wordCountNorm': min(1.0, signals.get('wordCount', 0) / 1500),
                'paragraphCountNorm': min(1.0, signals.get('paragraphCount', 0) / 12),
                'h2CountNorm': min(1.0, signals.get('h2Count', 0) / 8),
                'uniqueWordRatio': signals.get('uniqueWordRatio', 0),
                'referenceKeywordNorm': min(1.0, signals.get('referenceKeywordCount', 0) / 6),
                'hasH1Keyword': 1 if signals.get('h1ContainsKeyword') else 0,
                'hasUniqueTitle': 1 if signals.get('hasUniqueTitle') else 0,
                'hasVisibleDate': 1 if signals.get('hasVisibleDate') else 0,
                'metaDescriptionPresent': 1 if signals.get('hasMetaDescription') else 0,
                'canonicalPresent': 1 if signals.get('hasCanonical') else 0,
                'externalLinkPresent': 1 if signals.get('externalLinkCount', 0) > 0 else 0,
                'authorityLinkPresent': 1 if signals.get('externalAuthorityLinkCount', 0) > 0 else 0,
                'listPresent': 1 if signals.get('listCount', 0) > 0 else 0,
                'tablePresent': 1 if signals.get('tableCount', 0) > 0 else 0,
                'longParagraphPenalty': 1 if signals.get('paragraphAverageLength', 0) > 420 else 0,
                'avgSentenceLengthNorm': min(1.0, (signals.get('avgSentenceLength', 0) or 20) / 40),
                'depthLowFlag': 1 if flags.get('depthLow') else 0,
                'readabilityWeakFlag': 1 if flags.get('readabilityWeak') else 0,
                'actionableWeakFlag': 1 if flags.get('actionableWeak') else 0,
                'freshnessWeakFlag': 1 if flags.get('freshnessWeak') else 0,
                'titleMismatchFlag': 1 if flags.get('titleMismatch') else 0,

                'analysis_status': 'completed',
                'analysis_attempts': attempt,
            }

            print(f"      ‚úì Features extracted")
            return features

        except requests.exceptions.HTTPError as exc:
            status_code = exc.response.status_code if exc.response else None
            error_payload: Dict[str, Any] = {}
            error_message = str(exc)
            error_code = None
            if exc.response is not None:
                try:
                    error_payload = exc.response.json()
                    error_message = error_payload.get('error', error_message)
                    error_code = error_payload.get('code')
                except ValueError:
                    error_message = exc.response.text[:200]

            print(f"      ‚úó HTTP error analyzing URL: {error_message} (status {status_code})")

            last_error = {
                'analysis_status': 'failed',
                'analysis_error_kind': 'http_error',
                'analysis_http_status': status_code,
                'analysis_error_message': error_message,
                'analysis_error_code': error_code,
                'analysis_attempts': attempt,
            }

            if status_code == 429 and attempt < ANALYZE_RETRY_ATTEMPTS:
                wait_seconds = ANALYZE_RETRY_DELAY_SECONDS * attempt
                print(f"        ‚Üí Á≠âÂæÖ {wait_seconds:.0f} ÁßíÂæåÈáçË©¶ (ÈÅøÂÖçÈôêÊµÅ)")
                time.sleep(wait_seconds)
                continue
            if status_code in (500, 502, 503, 504) and attempt < ANALYZE_RETRY_ATTEMPTS:
                wait_seconds = ANALYZE_RETRY_DELAY_SECONDS * attempt
                print(f"        ‚Üí Á≠âÂæÖ {wait_seconds:.0f} ÁßíÂæåÈáçË©¶ (‰º∫ÊúçÂô®ÈåØË™§)")
                time.sleep(wait_seconds)
                continue
            break
        except requests.exceptions.RequestException as exc:
            error_message = str(exc)
            print(f"      ‚úó Error analyzing URL: {error_message}")
            last_error = {
                'analysis_status': 'failed',
                'analysis_error_kind': 'request_exception',
                'analysis_error_message': error_message,
                'analysis_attempts': attempt,
            }

            if attempt < ANALYZE_RETRY_ATTEMPTS:
                wait_seconds = ANALYZE_RETRY_DELAY_SECONDS * attempt
                print(f"        ‚Üí Á≠âÂæÖ {wait_seconds:.0f} ÁßíÂæåÈáçË©¶")
                time.sleep(wait_seconds)
                continue
            break

    if not last_error:
        last_error = {
            'analysis_status': 'failed',
            'analysis_error_kind': 'unknown',
            'analysis_error_message': 'ÂàÜÊûêÂ§±ÊïóÔºàÊú™Áü•ÂéüÂõ†Ôºâ',
            'analysis_attempts': ANALYZE_RETRY_ATTEMPTS,
        }

    return last_error


def collect_keywords(
    keywords: List[str],
    *,
    keyword_offset: int = 0,
    total_keywords: Optional[int] = None,
    persist_local: bool = True,
    sync_to_sheets: bool = True,
    update_status: bool = True,
    keyword_delay: Optional[float] = None,
    url_delay: Optional[float] = None,
    sheets_writer=None
) -> Dict[str, Any]:
    """ÈáùÂ∞çÊåáÂÆöÈóúÈçµÂ≠óÊâπÊ¨°ËíêÈõÜ SERP Ë≥áÊñô‰∏¶ÂõûÂÇ≥ÊëòË¶Å„ÄÇ"""
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    total_keywords = total_keywords if total_keywords is not None else max(keyword_offset + len(keywords), len(KEYWORDS))
    keyword_delay = KEYWORD_DELAY_SECONDS if keyword_delay is None else keyword_delay
    url_delay = URL_DELAY_SECONDS if url_delay is None else url_delay

    training_data = load_existing_data()
    seen_records = {record_signature(item['keyword'], item['url']) for item in training_data}
    keyword_counts = Counter(item['keyword'] for item in training_data)

    should_sync_to_sheets = sync_to_sheets and is_sheets_sync_enabled()

    writer = sheets_writer or (get_sheets_writer() if should_sync_to_sheets else None)
    processed_keywords = writer.get_processed_keywords() if writer else set()

    if training_data and persist_local:
        print(f"  ‚Üª Â∑≤ËºâÂÖ•Êó¢ÊúâË≥áÊñô {len(training_data)} Á≠ÜÔºà{len({item['keyword'] for item in training_data})} ÁµÑÈóúÈçµÂ≠óÔºâ")
    elif not training_data and persist_local:
        persist_progress(training_data, None, -1, keyword_total=total_keywords, persist_local=persist_local, update_status=update_status, sync_to_sheets=False, sheets_writer=writer)

    start_time = time.time()
    new_records = 0
    skipped_existing = 0
    errors: List[str] = []

    if persist_local:
        print(f"\n{'='*60}")
        print(f"SERP Data Collection - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*60}\n")

    for batch_index, keyword in enumerate(keywords):
        global_index = keyword_offset + batch_index
        if persist_local:
            print(f"\n[{global_index + 1}/{total_keywords}] Processing keyword: {keyword}")

        if writer and keyword in processed_keywords:
            if persist_local:
                print("  ‚Üª Â∑≤Ê®ôË®òÂÆåÊàêÔºåÁï•ÈÅéÈóúÈçµÂ≠ó")
            if update_status:
                update_status_file(training_data, keyword, global_index, total_keywords)
            continue

        if keyword_counts.get(keyword, 0) >= MIN_RESULTS_PER_KEYWORD:
            if persist_local:
                print(f"  ‚Üª Â∑≤Á¥ØÁ©ç {keyword_counts[keyword]} Á≠ÜÁµêÊûúÔºåÁï•ÈÅé SERP Âè´Áî®")
            if update_status:
                update_status_file(training_data, keyword, global_index, total_keywords)
            if writer:
                writer.mark_keyword_processed(keyword)
                processed_keywords.add(keyword)
            continue

        serp_results = fetch_serp_results(keyword)
        if not serp_results:
            if update_status:
                update_status_file(training_data, keyword, global_index, total_keywords)
            errors.append(f"SERP fetch failed: {keyword}")
            continue

        if keyword_delay and keyword_delay > 0 and batch_index > 0:
            time.sleep(keyword_delay)

        for result in serp_results:
            url = result['url']
            signature = record_signature(keyword, url)
            if signature in seen_records:
                skipped_existing += 1
                if persist_local:
                    print(f"      ‚Üª Â∑≤Â≠òÂú®Ë®òÈåÑÔºåÁï•ÈÅé {url[:60]}...")
                continue

            rank = result['rank']
            target_score = rank_to_score(rank)

            features = analyze_url(url, keyword)
            if not isinstance(features, dict):
                features = {
                    'analysis_status': 'failed',
                    'analysis_error_kind': 'invalid_response'
                }

            record = {
                'url': url,
                'keyword': keyword,
                'serp_rank': rank,
                'target_score': target_score,
                'title': result.get('title', ''),
                'features': features
            }

            training_data.append(record)
            seen_records.add(signature)
            keyword_counts[keyword] += 1
            new_records += 1

            persist_progress(
                training_data,
                keyword,
                global_index,
                keyword_total=total_keywords,
                persist_local=persist_local,
                update_status=update_status,
                sync_to_sheets=should_sync_to_sheets,
                sheets_writer=writer
            )

            if persist_local:
                print(f"      ‚úì Â∑≤ÂÑ≤Â≠òÔºàÁ¥ØË®à {len(training_data)} Á≠ÜÔºâ")

            if url_delay and url_delay > 0:
                time.sleep(url_delay)

        if writer:
            writer.mark_keyword_processed(keyword)
            processed_keywords.add(keyword)

    if update_status:
        persist_progress(
            training_data,
            None,
            keyword_offset + len(keywords),
            keyword_total=total_keywords,
            persist_local=persist_local,
            update_status=update_status,
            sync_to_sheets=False,
            sheets_writer=writer
        )

    elapsed = time.time() - start_time

    if persist_local:
        print(f"\n{'='*60}")
        print(f"Collection completed: {len(training_data)} records (Êñ∞Â¢û {new_records} Á≠ÜÔºåÁï•ÈÅé {skipped_existing} Á≠Ü)")
        if persist_local:
            print(f"Output files:")
            print(f"  - {OUTPUT_JSON}")
            print(f"  - {OUTPUT_CSV}")
            print(f"  - {STATUS_JSON}")
        print(f"ËÄóÊôÇÔºö{elapsed:.1f} Áßí")
        print(f"{'='*60}")

        manager = get_serp_manager()
        manager.print_status()

        tracker = get_cost_tracker()
        tracker.print_summary()
        tracker.export_csv()

    return {
        'keywordsProcessed': len(keywords),
        'newRecords': new_records,
        'skippedExisting': skipped_existing,
        'errors': errors,
        'elapsedSeconds': elapsed,
        'totalRecords': len(training_data)
    }


def collect_training_data():
    return collect_keywords(KEYWORDS)


if __name__ == '__main__':
    collect_training_data()
