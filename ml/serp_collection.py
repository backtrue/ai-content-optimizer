#!/usr/bin/env python3
"""
SERP Data Collection Script
Fetches top 10 results from ValueSerp for given keywords and extracts features via our /analyze API.
"""

import os
import json
import csv
import time
import requests
from typing import List, Dict, Optional, Tuple
from datetime import datetime
from serp_manager import get_manager as get_serp_manager
from cost_tracker import get_tracker as get_cost_tracker
from sheets_writer import get_sheets_writer


def load_env_variables() -> Optional[str]:
    """å˜—è©¦è¼‰å…¥æœ¬åœ°ç’°å¢ƒæª”ï¼Œè®“è…³æœ¬å¯ç›´æŽ¥ä½¿ç”¨ .env è¨­å®šã€‚"""
    candidate_files = ['.env.serp', '.env', '.env.serp.local', '.env.serp.example']

    def _parse_and_set(path: str) -> None:
        with open(path, 'r', encoding='utf-8') as handle:
            for raw_line in handle:
                line = raw_line.strip()
                if not line or line.startswith('#'):
                    continue
                if '=' not in line:
                    continue
                key, value = line.split('=', 1)
                key = key.strip()
                value = value.strip()
                if value and len(value) >= 2 and value[0] == value[-1] and value[0] in ('"', "'"):
                    value = value[1:-1]
                os.environ[key] = value

    for filename in candidate_files:
        if os.path.exists(filename):
            _parse_and_set(filename)
            print(f"âš™ï¸ å·²è¼‰å…¥ç’°å¢ƒè¨­å®šæª”ï¼š{filename}")
            return filename

    return None


load_env_variables()

# Configuration
SERPAPI_KEY = os.getenv('SERPAPI_KEY', '')
ANALYZE_API_URL = os.getenv('ANALYZE_API_URL', 'https://ragseo.thinkwithblack.com/api/analyze')
OUTPUT_DIR = './ml'
OUTPUT_CSV = os.path.join(OUTPUT_DIR, 'training_data.csv')
OUTPUT_JSON = os.path.join(OUTPUT_DIR, 'training_data.json')
STATUS_JSON = os.path.join(OUTPUT_DIR, 'collection_status.json')

# Rate limiting & retry configuration
KEYWORD_DELAY_SECONDS = float(os.getenv('SERP_KEYWORD_DELAY_SECONDS', '15'))
URL_DELAY_SECONDS = float(os.getenv('SERP_URL_DELAY_SECONDS', '12'))
ANALYZE_RETRY_ATTEMPTS = int(os.getenv('SERP_ANALYZE_RETRIES', '3'))
ANALYZE_RETRY_DELAY_SECONDS = float(os.getenv('SERP_ANALYZE_RETRY_DELAY_SECONDS', '20'))

# Keywords to analyze (100 keywords)
KEYWORDS = [
    "éžæ´²è±¬ç˜Ÿ", "å¼µå³»", "æ°´é¾åŸ", "mizkif", "ç²‰ç›’å¤§çŽ‹",
    "çŽ‰å±±é‡‘", "è¨±ç´¹é›„", "æ¨‚å¤©", "å¤©åœ°åŠå¿ƒ", "atlas",
    "å°ä¸­è³¼ç‰©ç¯€", "è—çœ¼æ·š", "ç‚Žäºžç¶¸", "åœ‹å¯¶", "å‘¨å­å®‰",
    "ä¸­è¯è·æ£’", "è‚‰è‚‰å¤§ç±³", "é„­æ™ºåŒ–", "exo", "mlbä¸–ç•Œå¤§è³½",
    "æ›¾é›…å¦®", "æž—åˆç«‹", "è©¹æ±Ÿæ‘", "äººæµ®æ–¼æ„›", "é¦¬å‚‘æ£®",
    "é«˜é€š", "æ™®ç™¼ä¸€è¬ç™»è¨˜", "ä¼¯æ©å®‰å¾·æ£®", "2025 mlb çƒå­£", "æ˜“çƒŠåƒç’½",
    "æ–°ç«¹åœæ°´", "æ´²ç¾Žåœ‹å°é å®šåœ°", "æ›²å¾·ç¾©", "æ˜Žå¤©çš„å¤©æ°£", "å®æ³°é›†åœ˜",
    "é„­æµ©å‡", "è¬æ²›æ©", "æ±Ÿå’Œæ¨¹", "ä¸­è¯è·æ£’ç›´æ’­", "å¹³é‡Žæƒ ä¸€",
    "é«˜é›„æ·é‹", "åœ‹çŽ‹ å° æ¹–äºº", "é«˜é€šè‚¡åƒ¹", "è”¡ç’§å", "è¬è–ç¯€",
    "å·´é»Žå¤§å¸«è³½", "cpblç›´æ’­", "å¤é”", "å¤§æ¦®è²¨é‹", "æ³°åœ‹åœ‹å–ª",
    "é«˜æ©‹è—", "austin reaves", "qcom", "å¨èƒ½å¸", "æ³°åœ‹",
    "æ³•åœ‹ç¾½çƒå…¬é–‹è³½", "é˜¿ä¿¡", "å‡±è’‚ä½©èŠ®", "ç™½æ™ä¹‹å¤œ", "yahoo",
    "å¾å¶”ç…Œ", "å¥½å‘³å°å§", "å°å—è—çœ¼æ·š", "å±±è±¬", "åŒå¿—å¤§éŠè¡Œ2025",
    "é»ƒå®‰", "æ¡ƒåœ’è¬è–åŸŽ", "ä½™å¾·é¾", "é»ƒé‡‘åƒ¹æ ¼", "ç‚¸è¨˜",
    "è³´é›…å¦", "å—æµ·", "é–ƒå…µ", "æ²³åŒ—å½©ä¼½ ig", "æ±Ÿå¤å®‡",
    "å¥³å­©", "æœ±æ‰¿æ´‹", "å…‰å¾©ç¯€ç”±ä¾†", "æ¶¼å±±ç‰¹å‹¤éšŠ", "euphoria",
    "nbaæˆ°ç¸¾", "00878", "ç²˜é‘«", "éŒ¦ç§€ç¤¾å€", "é¦¬åˆº å° ç±ƒç¶²",
    "ç°ç‹¼ å° æºœé¦¬", "é™³ä»¥ä¿¡", "persib bandung vs persis", "mlb fall classic 2025", "è¨±åŸºå®",
    "å…‰å¾©ç¯€", "æ™šå®‰å°é›ž", "f1", "æ‹“è’è€… å° å‹‡å£«", "å°é‡Žç”°ç´€ç¾Ž",
    "chatgpt atlas", "å‹‡å£« å° é‡‘å¡Š", "ç‰™è²·åŠ "
]


def load_existing_data() -> List[Dict]:
    """è¼‰å…¥æ—¢æœ‰è¨“ç·´è³‡æ–™ï¼Œè‹¥æœ¬åœ°æª”æ¡ˆä¸å­˜åœ¨å‰‡å˜—è©¦å¾ž Google Sheets è®€å–ã€‚"""
    if os.path.exists(OUTPUT_JSON):
        try:
            with open(OUTPUT_JSON, 'r', encoding='utf-8') as file:
                data = json.load(file)
                if isinstance(data, list):
                    return data
                print("âš ï¸ training_data.json æ ¼å¼éžé™£åˆ—ï¼Œå°‡å¿½ç•¥æ—¢æœ‰è³‡æ–™ã€‚")
        except json.JSONDecodeError:
            print("âš ï¸ training_data.json æ­£åœ¨å¯«å…¥æˆ–æ ¼å¼ä¸å®Œæ•´ï¼Œæš«æ™‚å¿½ç•¥æ—¢æœ‰è³‡æ–™ã€‚")
        except Exception as exc:
            print(f"âš ï¸ è¼‰å…¥æ—¢æœ‰è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{exc}")

    sheets_writer = get_sheets_writer()
    if not sheets_writer:
        return []

    try:
        sheet_records = sheets_writer.fetch_all_records()
        records: List[Dict] = []
        for row in sheet_records:
            features = {k: v for k, v in row.items() if k not in BASE_COLUMNS}
            record = {
                'url': row.get('url', ''),
                'keyword': row.get('keyword', ''),
                'serp_rank': row.get('serp_rank', ''),
                'target_score': row.get('target_score', ''),
                'title': row.get('title', ''),
                'features': features
            }
            records.append(record)
        if records:
            print(f"ðŸ§¾ å·²å¾ž Google Sheets è®€å…¥ {len(records)} ç­†æ—¢æœ‰è³‡æ–™")
        return records
    except Exception as exc:  # pylint: disable=broad-except
        print(f"âš ï¸ å¾ž Google Sheets è®€å–è³‡æ–™æ™‚å¤±æ•—ï¼š{exc}")
        return []


def save_csv(records: List[Dict]) -> None:
    """è¦†å¯«è¼¸å‡º CSVï¼Œä»¥ä¾¿å³æ™‚æª¢è¦–æœ€æ–°è³‡æ–™ã€‚"""
    if not records:
        if os.path.exists(OUTPUT_CSV):
            os.remove(OUTPUT_CSV)
        return

    feature_fields: List[str] = []
    for record in records:
        features = record.get('features', {})
        if not isinstance(features, dict):
            continue
        for key in features.keys():
            if key not in feature_fields:
                feature_fields.append(key)

    fieldnames = ['url', 'keyword', 'serp_rank', 'target_score', 'title'] + feature_fields
    with open(OUTPUT_CSV, 'w', newline='', encoding='utf-8') as csv_file:
        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)
        writer.writeheader()
        for record in records:
            row = {
                'url': record['url'],
                'keyword': record['keyword'],
                'serp_rank': record['serp_rank'],
                'target_score': record['target_score'],
                'title': record['title']
            }
            row.update(record.get('features', {}))
            writer.writerow(row)


def update_status_file(records: List[Dict], current_keyword: Optional[str], keyword_index: int) -> None:
    """è¼¸å‡ºè’é›†ç‹€æ…‹ JSONï¼Œæä¾›ç›£æŽ§è…³æœ¬èˆ‡äººå·¥å¿«é€Ÿæª¢è¦–ã€‚"""
    unique_keywords = {item['keyword'] for item in records}
    status_payload = {
        'timestamp': datetime.now().isoformat(),
        'total_records': len(records),
        'unique_keywords': len(unique_keywords),
        'current_keyword': current_keyword,
        'current_keyword_index': keyword_index,
        'progress_percent': round(len(unique_keywords) / len(KEYWORDS) * 100, 2) if KEYWORDS else 0,
        'remaining_keywords': max(0, len(KEYWORDS) - len(unique_keywords))
    }

    with open(STATUS_JSON, 'w', encoding='utf-8') as status_file:
        json.dump(status_payload, status_file, ensure_ascii=False, indent=2)


def persist_progress(records: List[Dict], current_keyword: Optional[str], keyword_index: int) -> None:
    """å³æ™‚å¯«å…¥ JSON/CSV ä¸¦æ›´æ–°ç‹€æ…‹æª”ï¼Œä¸¦å˜—è©¦åŒæ­¥è‡³ Google Sheetsã€‚"""
    with open(OUTPUT_JSON, 'w', encoding='utf-8') as json_file:
        json.dump(records, json_file, ensure_ascii=False, indent=2)

    save_csv(records)
    update_status_file(records, current_keyword, keyword_index)

    if current_keyword is None or not records:
        return

    sheets_writer = get_sheets_writer()
    if not sheets_writer:
        return

    try:
        sheets_writer.append_record(records[-1])
        print("  â†» å·²åŒæ­¥æœ€æ–°ç´€éŒ„è‡³ Google Sheets")
    except Exception as exc:  # pylint: disable=broad-except
        print(f"âš ï¸ å¯«å…¥ Google Sheets å¤±æ•—ï¼š{exc}")


def record_signature(keyword: str, url: str) -> Tuple[str, str]:
    return keyword, url


# SERP rank to score mapping
def rank_to_score(rank: int) -> int:
    """Convert SERP rank (1-10) to quality score (0-100)."""
    rank_score_map = {
        1: 100,
        2: 95,
        3: 90,
        4: 85,
        5: 80,
        6: 75,
        7: 70,
        8: 65,
        9: 60,
        10: 55
    }
    return rank_score_map.get(rank, 50)


def fetch_serp_results(keyword: str) -> List[Dict]:
    """Fetch top 10 SERP results using multi-service manager with automatic failover."""
    manager = get_serp_manager()
    tracker = get_cost_tracker()
    
    print(f"  Fetching SERP for: {keyword}")
    results, error, service = manager.fetch(keyword)
    
    # Track API usage
    service_name = service.lower().replace('api', '').replace('serp', '').strip() if service else 'unknown'
    tracker.record_request(service_name, success=(results is not None))
    
    if results:
        print(f"    âœ“ Got {len(results)} results from {service}")
        return results
    else:
        print(f"    âœ— Error: {error}")
        return []


def analyze_url(url: str, keyword: str) -> Optional[Dict]:
    """Call our /analyze API to extract features for a URL, with retry support."""
    if not url:
        return None
    
    payload = {
        'contentUrl': url,
        'targetKeywords': [keyword],
        'returnChunks': False
    }
    
    for attempt in range(1, ANALYZE_RETRY_ATTEMPTS + 1):
        try:
            print(f"    Analyzing: {url[:60]}... (attempt {attempt}/{ANALYZE_RETRY_ATTEMPTS})")
            response = requests.post(ANALYZE_API_URL, json=payload, timeout=60)
            response.raise_for_status()
            data = response.json()
            
            # Extract features from response
            signals = data.get('contentSignals', {})
            flags = data.get('scoreGuards', {}).get('contentQualityFlags', {})
            hcu_counts = data.get('scoreGuards', {}).get('hcuCounts', {})
            
            # è¨ˆç®— HCU ç›¸é—œç‰¹å¾µ
            total_hcu = hcu_counts.get('yes', 0) + hcu_counts.get('partial', 0) + hcu_counts.get('no', 0)
            hcu_yes_ratio = hcu_counts.get('yes', 0) / total_hcu if total_hcu > 0 else 0
            hcu_partial_ratio = hcu_counts.get('partial', 0) / total_hcu if total_hcu > 0 else 0
            hcu_no_ratio = hcu_counts.get('no', 0) / total_hcu if total_hcu > 0 else 0
            hcu_content_helpfulness = (hcu_yes_ratio + hcu_partial_ratio * 0.5) if total_hcu > 0 else 0
            
            # å…§å®¹çµæ§‹ç‰¹å¾µ
            qa_format_score = min(1.0, signals.get('qaFormatScore', 0))
            first_para_answer_quality = min(1.0, signals.get('firstParagraphAnswerQuality', 0))
            semantic_paragraph_focus = min(1.0, signals.get('semanticParagraphFocus', 0))
            heading_hierarchy_quality = min(1.0, signals.get('headingHierarchyQuality', 0))
            topic_cohesion = min(1.0, signals.get('topicCohesion', 0))
            
            # æŠ€è¡“èˆ‡æ¨™è¨˜ç‰¹å¾µ
            faq_schema_present = 1 if signals.get('faqSchemaPresent') else 0
            howto_schema_present = 1 if signals.get('howtoSchemaPresent') else 0
            article_schema_present = 1 if signals.get('articleSchemaPresent') else 0
            organization_schema_present = 1 if signals.get('organizationSchemaPresent') else 0
            og_tags_complete = min(1.0, signals.get('ogTagsComplete', 0))
            meta_tags_quality = min(1.0, signals.get('metaTagsQuality', 0))
            html_structure_validity = min(1.0, signals.get('htmlStructureValidity', 0))
            
            # å“ç‰Œå¯¦é«”èˆ‡ä¿¡ä»»ç‰¹å¾µ
            author_info_present = 1 if signals.get('authorInfoPresent') else 0
            brand_entity_clarity = min(1.0, signals.get('brandEntityClarity', 0))
            external_citation_count = min(1.0, signals.get('externalCitationCount', 0) / 10)
            social_media_links_present = 1 if signals.get('socialMediaLinksPresent') else 0
            review_rating_present = 1 if signals.get('reviewRatingPresent') else 0
            
            # AI æœå°‹é©é…ç‰¹å¾µ
            semantic_naturalness = min(1.0, signals.get('semanticNaturalness', 0))
            paragraph_extractability = min(1.0, signals.get('paragraphExtractability', 0))
            rich_snippet_format = min(1.0, signals.get('richSnippetFormat', 0))
            citability_trust_score = min(1.0, signals.get('citabilityTrustScore', 0))
            multimedia_support = min(1.0, signals.get('multimediaSupport', 0))
            
            # ä¿ç•™æ—¢æœ‰çš„åŸºç¤Žç‰¹å¾µï¼ˆå‘å¾Œç›¸å®¹ï¼‰
            features = {
                # HCU ç‰¹å¾µ
                'hcuYesRatio': hcu_yes_ratio,
                'hcuPartialRatio': hcu_partial_ratio,
                'hcuNoRatio': hcu_no_ratio,
                'hcuContentHelpfulness': hcu_content_helpfulness,
                
                # å…§å®¹çµæ§‹
                'qaFormatScore': qa_format_score,
                'firstParagraphAnswerQuality': first_para_answer_quality,
                'semanticParagraphFocus': semantic_paragraph_focus,
                'headingHierarchyQuality': heading_hierarchy_quality,
                'topicCohesion': topic_cohesion,
                
                # æŠ€è¡“èˆ‡æ¨™è¨˜
                'faqSchemaPresent': faq_schema_present,
                'howtoSchemaPresent': howto_schema_present,
                'articleSchemaPresent': article_schema_present,
                'organizationSchemaPresent': organization_schema_present,
                'ogTagsComplete': og_tags_complete,
                'metaTagsQuality': meta_tags_quality,
                'htmlStructureValidity': html_structure_validity,
                
                # å“ç‰Œå¯¦é«”èˆ‡ä¿¡ä»»
                'authorInfoPresent': author_info_present,
                'brandEntityClarity': brand_entity_clarity,
                'externalCitationCount': external_citation_count,
                'socialMediaLinksPresent': social_media_links_present,
                'reviewRatingPresent': review_rating_present,
                
                # AI æœå°‹é©é…
                'semanticNaturalness': semantic_naturalness,
                'paragraphExtractability': paragraph_extractability,
                'richSnippetFormat': rich_snippet_format,
                'citabilityTrustScore': citability_trust_score,
                'multimediaSupport': multimedia_support,
                
                # ä¿ç•™æ—¢æœ‰ç‰¹å¾µï¼ˆå‘å¾Œç›¸å®¹ï¼‰
                'wordCountNorm': min(1.0, signals.get('wordCount', 0) / 1500),
                'paragraphCountNorm': min(1.0, signals.get('paragraphCount', 0) / 12),
                'h2CountNorm': min(1.0, signals.get('h2Count', 0) / 8),
                'uniqueWordRatio': signals.get('uniqueWordRatio', 0),
                'referenceKeywordNorm': min(1.0, signals.get('referenceKeywordCount', 0) / 6),
                'hasH1Keyword': 1 if signals.get('h1ContainsKeyword') else 0,
                'hasUniqueTitle': 1 if signals.get('hasUniqueTitle') else 0,
                'hasVisibleDate': 1 if signals.get('hasVisibleDate') else 0,
                'metaDescriptionPresent': 1 if signals.get('hasMetaDescription') else 0,
                'canonicalPresent': 1 if signals.get('hasCanonical') else 0,
                'externalLinkPresent': 1 if signals.get('externalLinkCount', 0) > 0 else 0,
                'authorityLinkPresent': 1 if signals.get('externalAuthorityLinkCount', 0) > 0 else 0,
                'listPresent': 1 if signals.get('listCount', 0) > 0 else 0,
                'tablePresent': 1 if signals.get('tableCount', 0) > 0 else 0,
                'longParagraphPenalty': 1 if signals.get('paragraphAverageLength', 0) > 420 else 0,
                'avgSentenceLengthNorm': min(1.0, (signals.get('avgSentenceLength', 0) or 20) / 40),
                'depthLowFlag': 1 if flags.get('depthLow') else 0,
                'readabilityWeakFlag': 1 if flags.get('readabilityWeak') else 0,
                'actionableWeakFlag': 1 if flags.get('actionableWeak') else 0,
                'freshnessWeakFlag': 1 if flags.get('freshnessWeak') else 0,
                'titleMismatchFlag': 1 if flags.get('titleMismatch') else 0,
            }
            
            print(f"      âœ“ Features extracted")
            return features
        
        except requests.exceptions.HTTPError as e:
            status_code = e.response.status_code if e.response else None
            print(f"      âœ— HTTP error analyzing URL: {e} (status {status_code})")
            if status_code == 429 and attempt < ANALYZE_RETRY_ATTEMPTS:
                wait_seconds = ANALYZE_RETRY_DELAY_SECONDS * attempt
                print(f"        â†’ ç­‰å¾… {wait_seconds:.0f} ç§’å¾Œé‡è©¦ (é¿å…é™æµ)")
                time.sleep(wait_seconds)
                continue
            if status_code in (500, 502, 503, 504) and attempt < ANALYZE_RETRY_ATTEMPTS:
                wait_seconds = ANALYZE_RETRY_DELAY_SECONDS * attempt
                print(f"        â†’ ç­‰å¾… {wait_seconds:.0f} ç§’å¾Œé‡è©¦ (ä¼ºæœå™¨éŒ¯èª¤)")
                time.sleep(wait_seconds)
                continue
            return None
        except requests.exceptions.RequestException as e:
            print(f"      âœ— Error analyzing URL: {e}")
            if attempt < ANALYZE_RETRY_ATTEMPTS:
                wait_seconds = ANALYZE_RETRY_DELAY_SECONDS * attempt
                print(f"        â†’ ç­‰å¾… {wait_seconds:.0f} ç§’å¾Œé‡è©¦")
                time.sleep(wait_seconds)
                continue
            return None
    
    return None


def collect_training_data():
    """Main collection flow."""
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    training_data = load_existing_data()
    seen_records = {record_signature(item['keyword'], item['url']) for item in training_data}
    if training_data:
        print(f"  â†» å·²è¼‰å…¥æ—¢æœ‰è³‡æ–™ {len(training_data)} ç­†ï¼ˆ{len({item['keyword'] for item in training_data})} çµ„é—œéµå­—ï¼‰")
    else:
        persist_progress(training_data, None, -1)

    print(f"\n{'='*60}")
    print(f"SERP Data Collection - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")

    for keyword_index, keyword in enumerate(KEYWORDS):
        print(f"\n[{keyword_index + 1}/{len(KEYWORDS)}] Processing keyword: {keyword}")

        serp_results = fetch_serp_results(keyword)
        if not serp_results:
            update_status_file(training_data, keyword, keyword_index)
            continue

        if KEYWORD_DELAY_SECONDS > 0 and keyword_index > 0:
            time.sleep(KEYWORD_DELAY_SECONDS)  # Rate limiting between keywords

        for result in serp_results:
            url = result['url']
            signature = record_signature(keyword, url)
            if signature in seen_records:
                print(f"      â†» å·²å­˜åœ¨è¨˜éŒ„ï¼Œç•¥éŽ {url[:60]}...")
                continue

            rank = result['rank']
            target_score = rank_to_score(rank)

            features = analyze_url(url, keyword)
            if not features:
                continue

            record = {
                'url': url,
                'keyword': keyword,
                'serp_rank': rank,
                'target_score': target_score,
                'title': result.get('title', ''),
                'features': features
            }

            training_data.append(record)
            seen_records.add(signature)
            persist_progress(training_data, keyword, keyword_index)
            print(f"      âœ“ å·²å„²å­˜ï¼ˆç´¯è¨ˆ {len(training_data)} ç­†ï¼‰")

            if URL_DELAY_SECONDS > 0:
                time.sleep(URL_DELAY_SECONDS)  # Rate limiting between URLs

    persist_progress(training_data, None, len(KEYWORDS))

    print(f"\n{'='*60}")
    print(f"Collection completed: {len(training_data)} records")
    print(f"Output files:")
    print(f"  - {OUTPUT_JSON}")
    print(f"  - {OUTPUT_CSV}")
    print(f"  - {STATUS_JSON}")
    print(f"{'='*60}")

    # Print SERP manager status
    manager = get_serp_manager()
    manager.print_status()

    # Print cost summary
    tracker = get_cost_tracker()
    tracker.print_summary()
    tracker.export_csv()

if __name__ == '__main__':
    collect_training_data()
