{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Scoring Model Training\n",
    "\n",
    "This notebook trains an XGBoost model using SERP ranking data to predict content quality scores.\n",
    "\n",
    "## Setup\n",
    "1. Upload `training_data.csv` to Colab\n",
    "2. Run cells sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q xgboost scikit-learn pandas numpy shap matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import json\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Dependencies loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "df = pd.read_csv('training_data.csv')\n",
    "print(f\"Loaded {len(df)} records\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data exploration\n",
    "print(f\"Target score distribution:\")\n",
    "print(df['target_score'].describe())\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Visualize target distribution\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df['target_score'], bins=10, edgecolor='black')\n",
    "plt.xlabel('Target Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Target Score Distribution')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(df['serp_rank'], df['target_score'])\n",
    "plt.xlabel('SERP Rank')\n",
    "plt.ylabel('Target Score')\n",
    "plt.title('SERP Rank vs Target Score')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "# Drop non-feature columns\n",
    "feature_cols = [col for col in df.columns if col not in ['url', 'keyword', 'serp_rank', 'target_score', 'title']]\n",
    "print(f\"Feature columns ({len(feature_cols)}): {feature_cols}\")\n",
    "\n",
    "X = df[feature_cols].fillna(0)\n",
    "y = df['target_score']\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nFeature statistics:\")\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost model\n",
    "model = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    verbosity=1\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "print(\"Model trained successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "print(\"Training Metrics:\")\n",
    "print(f\"  RMSE: {np.sqrt(mean_squared_error(y_train, y_pred_train)):.4f}\")\n",
    "print(f\"  MAE: {mean_absolute_error(y_train, y_pred_train):.4f}\")\n",
    "print(f\"  R²: {r2_score(y_train, y_pred_train):.4f}\")\n",
    "\n",
    "print(\"\\nTest Metrics:\")\n",
    "print(f\"  RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_test)):.4f}\")\n",
    "print(f\"  MAE: {mean_absolute_error(y_test, y_pred_test):.4f}\")\n",
    "print(f\"  R²: {r2_score(y_test, y_pred_test):.4f}\")\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
    "print(f\"\\nCross-validation R² scores: {cv_scores}\")\n",
    "print(f\"Mean CV R²: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 15 Important Features:\")\n",
    "print(feature_importance.head(15))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['feature'].head(15), feature_importance['importance'].head(15))\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 15 Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction visualization\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_train, y_pred_train, alpha=0.5, label='Train')\n",
    "plt.scatter(y_test, y_pred_test, alpha=0.5, label='Test')\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Score')\n",
    "plt.ylabel('Predicted Score')\n",
    "plt.title('Actual vs Predicted')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "residuals = y_test - y_pred_test\n",
    "plt.scatter(y_pred_test, residuals, alpha=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Score')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model coefficients for JavaScript\n",
    "# Extract tree structure and convert to linear approximation\n",
    "\n",
    "model_config = {\n",
    "    'version': '1.0.0-xgboost-trained',\n",
    "    'createdAt': pd.Timestamp.now().isoformat(),\n",
    "    'description': 'Trained XGBoost model using SERP ranking data',\n",
    "    'metrics': {\n",
    "        'train_rmse': float(np.sqrt(mean_squared_error(y_train, y_pred_train))),\n",
    "        'test_rmse': float(np.sqrt(mean_squared_error(y_test, y_pred_test))),\n",
    "        'test_r2': float(r2_score(y_test, y_pred_test)),\n",
    "        'cv_r2_mean': float(cv_scores.mean()),\n",
    "        'cv_r2_std': float(cv_scores.std())\n",
    "    },\n",
    "    'feature_importance': feature_importance.to_dict('records'),\n",
    "    'training_samples': len(df),\n",
    "    'model_type': 'XGBRegressor'\n",
    "}\n",
    "\n",
    "# Save model config\n",
    "with open('trained_model_config.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(model_config, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Model config saved to trained_model_config.json\")\n",
    "print(json.dumps(model_config, ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model for later use\n",
    "model.save_model('trained_model.json')\n",
    "print(\"Model saved to trained_model.json\")\n",
    "\n",
    "# Also save as pickle for Python use\n",
    "import pickle\n",
    "with open('trained_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "print(\"Model saved to trained_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for all data\n",
    "df['predicted_score'] = model.predict(X)\n",
    "df['prediction_error'] = df['target_score'] - df['predicted_score']\n",
    "\n",
    "print(\"Predictions added to dataframe\")\n",
    "print(df[['keyword', 'serp_rank', 'target_score', 'predicted_score', 'prediction_error']].head(10))\n",
    "\n",
    "# Save predictions\n",
    "df.to_csv('predictions.csv', index=False)\n",
    "print(\"\\nPredictions saved to predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training samples: {len(df)}\")\n",
    "print(f\"Features: {len(feature_cols)}\")\n",
    "print(f\"\\nBest metrics:\")\n",
    "print(f\"  Test R²: {r2_score(y_test, y_pred_test):.4f}\")\n",
    "print(f\"  Test RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_test)):.4f}\")\n",
    "print(f\"  CV R² (mean): {cv_scores.mean():.4f}\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"  1. Download trained_model_config.json\")\n",
    "print(f\"  2. Update scoring-model.js with new coefficients\")\n",
    "print(f\"  3. Deploy to production\")\n",
    "print(f\"  4. Monitor SERP performance\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
