#!/usr/bin/env python3
"""
æ¨¡å‹è¨“ç·´ CLI å·¥å…·
æ”¯æ´éäº’å‹•å¼è¨“ç·´ã€æ¨¡å‹è½‰æª”ã€è‡ªå‹•éƒ¨ç½²
"""

import os
import re
import sys
import json
import math
import argparse
import pickle
import tempfile
import subprocess
import urllib.request
import urllib.error
from collections import Counter, defaultdict
from datetime import datetime, timezone, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
import numpy as np
from sklearn.ensemble import XGBRegressor
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error


FEATURE_REGISTRY = [
    # HCU ratios
    ('hcuYesRatio', {'type': 'ratio'}),
    ('hcuPartialRatio', {'type': 'ratio'}),
    ('hcuNoRatio', {'type': 'ratio'}),
    # Intent / narrative quality
    ('titleIntentMatch', {'type': 'ratio'}),
    ('firstParagraphAnswerQuality', {'type': 'ratio'}),
    ('qaFormatScore', {'type': 'ratio'}),
    ('topicCohesion', {'type': 'ratio'}),
    ('semanticParagraphFocus', {'type': 'ratio'}),
    ('semanticNaturalness', {'type': 'ratio'}),
    ('paragraphExtractability', {'type': 'ratio'}),
    ('richSnippetFormat', {'type': 'ratio'}),
    ('citabilityTrustScore', {'type': 'ratio'}),
    ('multimediaSupport', {'type': 'ratio'}),
    ('inspectability', {'type': 'ratio'}),
    # Pre-normalized features
    ('wordCountNorm', {'type': 'ratio'}),
    ('referenceKeywordNorm', {'type': 'ratio'}),
    ('actionableScoreNorm', {'type': 'ratio'}),
    ('avgSentenceLengthNorm', {'type': 'ratio'}),
    ('longParagraphPenalty', {'type': 'ratio'}),
    ('brandEntityClarity', {'type': 'ratio'}),
    # Counts (min-max or log scaling)
    ('wordCount', {'type': 'minmax', 'max': 2500}),
    ('paragraphCount', {'type': 'minmax', 'max': 60}),
    ('longParagraphCount', {'type': 'minmax', 'max': 30}),
    ('paragraphAverageLength', {'type': 'inverse_minmax', 'min': 60, 'max': 250}),
    ('avgSentenceLength', {'type': 'inverse_minmax', 'min': 12, 'max': 40}),
    ('listCount', {'type': 'minmax', 'max': 30}),
    ('tableCount', {'type': 'minmax', 'max': 15}),
    ('imageCount', {'type': 'minmax', 'max': 40}),
    ('imageWithAltCount', {'type': 'minmax', 'max': 40}),
    ('externalCitationCount', {'type': 'log', 'scale': 25}),
    ('externalAuthorityLinkCount', {'type': 'log', 'scale': 20}),
    ('externalLinkCount', {'type': 'log', 'scale': 40}),
    ('evidenceCount', {'type': 'log', 'scale': 15}),
    ('recentYearCount', {'type': 'minmax', 'max': 5}),
    ('experienceCueCount', {'type': 'minmax', 'max': 12}),
    ('caseStudyCount', {'type': 'minmax', 'max': 8}),
    ('h1Count', {'type': 'minmax', 'max': 4}),
    ('h2Count', {'type': 'minmax', 'max': 15}),
    ('actionableStepCount', {'type': 'minmax', 'max': 25}),
    ('actionableScore', {'type': 'minmax', 'max': 12}),
    # Unique and experience signals
    ('uniqueWordRatio', {'type': 'ratio'}),
    ('experienceCueNorm', {'type': 'ratio'}),
    ('entityRichnessNorm', {'type': 'ratio'}),
    # Boolean presence flags
    ('authorInfoPresent', {'type': 'boolean'}),
    ('socialMediaLinksPresent', {'type': 'boolean'}),
    ('reviewRatingPresent', {'type': 'boolean'}),
    ('hasFirstPersonNarrative', {'type': 'boolean'}),
    ('hasAuthorInfo', {'type': 'boolean'}),
    ('hasPublisherInfo', {'type': 'boolean'}),
    ('hasPublishedDate', {'type': 'boolean'}),
    ('hasModifiedDate', {'type': 'boolean'}),
    ('hasVisibleDate', {'type': 'boolean'}),
    ('hasFaqSchema', {'type': 'boolean'}),
    ('hasHowToSchema', {'type': 'boolean'}),
    ('hasArticleSchema', {'type': 'boolean'}),
    ('hasOrganizationSchema', {'type': 'boolean'}),
    ('hasCanonical', {'type': 'boolean'}),
    ('hasMetaDescription', {'type': 'boolean'}),
    ('hasChecklistLanguage', {'type': 'boolean'}),
    ('hasNumberedSteps', {'type': 'boolean'})
]

FEATURE_SPECS = {name: spec for name, spec in FEATURE_REGISTRY}
FEATURE_NAMES = [name for name, _ in FEATURE_REGISTRY]


def clamp(value: float, min_value: float = 0.0, max_value: float = 1.0) -> float:
    return max(min_value, min(max_value, float(value)))


def coerce_score(value: Any) -> Optional[float]:
    if value is None:
        return None
    try:
        if isinstance(value, str):
            value = float(value.strip())
        elif isinstance(value, (int, float)):
            value = float(value)
        else:
            return None
    except (ValueError, TypeError):
        return None

    if not math.isfinite(value):
        return None
    return value


def resolve_feature_value(record: Dict, name: str) -> Any:
    features = record.get('features') or {}
    if name in features:
        return features.get(name)
    # å˜—è©¦å›é€€åˆ°é ‚å±¤æ¬„ä½
    return record.get(name)


def normalize_feature(name: str, value: Any) -> float:
    spec = FEATURE_SPECS.get(name)
    if spec is None or value is None:
        return 0.0

    try:
        if spec['type'] == 'ratio':
            return clamp(value)
        if spec['type'] == 'boolean':
            if isinstance(value, bool):
                return 1.0 if value else 0.0
            if isinstance(value, (int, float)):
                return 1.0 if value else 0.0
            if isinstance(value, str):
                return 1.0 if value.strip().lower() in {'true', '1', 'yes'} else 0.0
            return 0.0
        if spec['type'] == 'minmax':
            max_val = spec.get('max', 1.0) or 1.0
            if max_val <= 0:
                max_val = 1.0
            return clamp(float(value) / max_val)
        if spec['type'] == 'inverse_minmax':
            min_val = spec.get('min', 0.0)
            max_val = spec.get('max', 1.0)
            span = max(max_val - min_val, 1e-6)
            score = 1.0 - (float(value) - min_val) / span
            return clamp(score)
        if spec['type'] == 'log':
            scale = spec.get('scale', 10.0) or 10.0
            return clamp(math.log1p(max(0.0, float(value))) / math.log1p(scale))
    except (ValueError, TypeError):
        return 0.0

    return 0.0

# å‡è¨­å·²æœ‰çš„è¨“ç·´è³‡æ–™è¼‰å…¥å‡½æ•¸
def load_training_data(data_dir: str) -> Tuple[List[Dict], np.ndarray, np.ndarray]:
    """
    å¾æŒ‡å®šç›®éŒ„è¼‰å…¥è¨“ç·´è³‡æ–™
    
    Args:
        data_dir: è¨“ç·´è³‡æ–™ç›®éŒ„ï¼ˆåŒ…å« CSV/JSON æª”æ¡ˆï¼‰
    
    Returns:
        (records, X, y) - è¨˜éŒ„ã€ç‰¹å¾µã€æ¨™ç±¤
    """
    data_path = Path(data_dir)
    
    if not data_path.exists():
        raise FileNotFoundError(f"è³‡æ–™ç›®éŒ„ä¸å­˜åœ¨: {data_dir}")
    
    records = []
    
    # è¼‰å…¥ JSON æª”æ¡ˆ
    json_files = list(data_path.glob('*.json'))
    for json_file in json_files:
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if isinstance(data, list):
                    records.extend(data)
                elif isinstance(data, dict) and 'records' in data:
                    records.extend(data['records'])
        except Exception as e:
            print(f"âš ï¸ è¼‰å…¥ {json_file} å¤±æ•—: {e}")
    
    if not records:
        raise ValueError(f"æœªæ‰¾åˆ°è¨“ç·´è³‡æ–™: {data_dir}")
    
    cleaned_records, cleaning_stats = clean_training_records(records)
    print(
        "âœ… å·²è¼‰å…¥ {after} ç­†è¨“ç·´è¨˜éŒ„ (ç§»é™¤ {dropped} ç­†ç„¡æ•ˆï¼Œé‡è¤‡ {dups} ç­†)".format(
            after=cleaning_stats['after_deduplicate'],
            dropped=cleaning_stats['dropped_invalid'],
            dups=cleaning_stats['deduplicated']
        )
    )
    
    # æå–ç‰¹å¾µèˆ‡æ¨™ç±¤
    X, y = extract_features_and_labels(cleaned_records)
    
    return cleaned_records, X, y


def extract_features_and_labels(records: List[Dict]) -> Tuple[np.ndarray, np.ndarray]:
    """
    å¾è¨˜éŒ„ä¸­æå–ç‰¹å¾µèˆ‡æ¨™ç±¤
    
    Args:
        records: è¨“ç·´è¨˜éŒ„æ¸…å–®
    
    Returns:
        (X, y) - ç‰¹å¾µçŸ©é™£èˆ‡æ¨™ç±¤å‘é‡
    """
    # å®šç¾©ç‰¹å¾µæ¬„ä½
    X_list = []
    y_list = []
    
    for record in records:
        feature_vector = []
        for name in FEATURE_NAMES:
            raw_value = resolve_feature_value(record, name)
            normalized = normalize_feature(name, raw_value)
            feature_vector.append(normalized)

        target_score = coerce_score(record.get('target_score'))
        if target_score is not None:
            X_list.append(feature_vector)
            y_list.append(target_score)
    
    if not X_list:
        raise ValueError("æœªæ‰¾åˆ°æœ‰æ•ˆçš„è¨“ç·´è³‡æ–™")
    
    X = np.array(X_list)
    y = np.array(y_list)
    
    print(f"âœ… ç‰¹å¾µæå–å®Œæˆ: {X.shape[0]} ç­†è¨˜éŒ„, {X.shape[1]} å€‹ç‰¹å¾µ")
    
    return X, y


def clean_training_records(records: List[Dict]) -> Tuple[List[Dict], Dict[str, int]]:
    stats = Counter(
        dropped_invalid=0,
        dropped_invalid_score=0,
        dropped_out_of_range=0,
        deduplicated=0,
        after_deduplicate=0
    )

    dedup_map: Dict[Tuple[str, str], Dict] = {}
    fallback_base = datetime.now(timezone.utc)

    for idx, record in enumerate(records):
        score = coerce_score(record.get('target_score'))
        if score is None:
            stats['dropped_invalid_score'] += 1
            stats['dropped_invalid'] += 1
            continue
        if score < 0 or score > 100:
            stats['dropped_out_of_range'] += 1
            stats['dropped_invalid'] += 1
            continue

        normalized = dict(record)
        normalized['target_score'] = float(score)

        keyword = safe_lower(normalized.get('keyword'))
        url = safe_lower(normalized.get('url'))
        key = (keyword, url) if keyword or url else (f'__idx_{idx}', '')

        timestamp = extract_timestamp(normalized)
        if timestamp is None:
            timestamp = fallback_base + timedelta(seconds=idx)
        normalized['_ts'] = timestamp

        existing = dedup_map.get(key)
        if existing is None or existing['_ts'] <= timestamp:
            if existing is not None:
                stats['deduplicated'] += 1
            dedup_map[key] = normalized
        else:
            stats['deduplicated'] += 1

    cleaned: List[Dict] = []
    for record in dedup_map.values():
        record.pop('_ts', None)
        cleaned.append(record)

    stats['after_deduplicate'] = len(cleaned)
    return cleaned, stats


def safe_lower(value: Optional[str]) -> str:
    if isinstance(value, str):
        return value.strip().lower()
    return ''


def feature_present(features: Dict, name: str) -> bool:
    if name not in features:
        return False
    value = features.get(name)
    if value is None:
        return False
    if isinstance(value, str) and not value.strip():
        return False
    return True


def train_model(
    X: np.ndarray,
    y: np.ndarray,
    test_size: float = 0.2,
    records: Optional[List[Dict]] = None,
    time_split: bool = False
) -> Tuple[XGBRegressor, Dict]:
    """
    è¨“ç·´ XGBoost æ¨¡å‹
    
    Args:
        X: ç‰¹å¾µçŸ©é™£
        y: æ¨™ç±¤å‘é‡
        test_size: æ¸¬è©¦é›†æ¯”ä¾‹
    
    Returns:
        (model, metrics) - è¨“ç·´å¥½çš„æ¨¡å‹èˆ‡è©•ä¼°æŒ‡æ¨™
    """
    print("ğŸ¤– é–‹å§‹è¨“ç·´æ¨¡å‹...")
    
    # åˆ†å‰²è¨“ç·´/æ¸¬è©¦é›†
    train_indices, test_indices = split_train_test_indices(
        total=len(X),
        test_size=test_size,
        records=records,
        time_split=time_split
    )

    X_train, X_test = X[train_indices], X[test_indices]
    y_train, y_test = y[train_indices], y[test_indices]
    
    print(f"  è¨“ç·´é›†: {X_train.shape[0]} ç­†")
    print(f"  æ¸¬è©¦é›†: {X_test.shape[0]} ç­†")
    
    # è¨“ç·´æ¨¡å‹
    model = create_regressor()
    
    model.fit(X_train, y_train)
    
    # è©•ä¼°æ¨¡å‹
    y_pred_train = model.predict(X_train)
    y_pred_test = model.predict(X_test)
    
    metrics = {
        'train_mse': float(mean_squared_error(y_train, y_pred_train)),
        'test_mse': float(mean_squared_error(y_test, y_pred_test)),
        'train_rmse': float(np.sqrt(mean_squared_error(y_train, y_pred_train))),
        'test_rmse': float(np.sqrt(mean_squared_error(y_test, y_pred_test))),
        'train_mae': float(mean_absolute_error(y_train, y_pred_train)),
        'test_mae': float(mean_absolute_error(y_test, y_pred_test)),
        'train_r2': float(r2_score(y_train, y_pred_train)),
        'test_r2': float(r2_score(y_test, y_pred_test))
    }
    
    print(f"âœ… æ¨¡å‹è¨“ç·´å®Œæˆ")
    print(f"  è¨“ç·´ RMSE: {metrics['train_rmse']:.4f}")
    print(f"  æ¸¬è©¦ RMSE: {metrics['test_rmse']:.4f}")
    print(f"  è¨“ç·´ RÂ²: {metrics['train_r2']:.4f}")
    print(f"  æ¸¬è©¦ RÂ²: {metrics['test_r2']:.4f}")
    
    return model, metrics


def create_regressor() -> XGBRegressor:
    return XGBRegressor(
        n_estimators=100,
        max_depth=6,
        learning_rate=0.1,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        n_jobs=-1
    )


def split_train_test_indices(
    total: int,
    test_size: float,
    records: Optional[List[Dict]] = None,
    time_split: bool = False
) -> Tuple[np.ndarray, np.ndarray]:
    if total < 2:
        return np.array([0]), np.array([])

    test_size = min(max(test_size, 0.05), 0.5)

    if time_split and records:
        train_idx, test_idx = split_indices_by_time(records, test_size)
        return np.array(train_idx), np.array(test_idx)

    indices = np.arange(total)
    train_idx, test_idx = train_test_split(indices, test_size=test_size, random_state=42)
    return train_idx, test_idx


def split_indices_by_time(records: List[Dict], test_size: float) -> Tuple[List[int], List[int]]:
    dated = []
    fallback_base = datetime.fromtimestamp(0, tz=timezone.utc)

    for idx, record in enumerate(records):
        ts = extract_timestamp(record)
        if ts is None:
            ts = fallback_base + (idx * (datetime.fromtimestamp(1, tz=timezone.utc) - fallback_base))
        dated.append((idx, ts))

    dated.sort(key=lambda item: item[1])
    split_point = max(1, min(len(dated) - 1, int(len(dated) * (1 - test_size))))
    train_indices = [idx for idx, _ in dated[:split_point]]
    test_indices = [idx for idx, _ in dated[split_point:]]
    return train_indices, test_indices


def extract_timestamp(record: Dict) -> Optional[datetime]:
    possible_fields = [
        record.get('timestamp'),
        record.get('createdAt'),
        record.get('analyzedAt'),
        record.get('updatedAt')
    ]

    for value in possible_fields:
        if value is None:
            continue
        if isinstance(value, (int, float)):
            try:
                return datetime.fromtimestamp(float(value), tz=timezone.utc)
            except (OverflowError, OSError):
                continue
        if isinstance(value, str) and value.strip():
            ts_str = value.strip()
            if ts_str.endswith('Z'):
                ts_str = ts_str[:-1] + '+00:00'
            try:
                return datetime.fromisoformat(ts_str)
            except ValueError:
                continue
    return None


def run_kfold_evaluation(X: np.ndarray, y: np.ndarray, folds: int) -> Dict:
    kf = KFold(n_splits=folds, shuffle=True, random_state=42)
    fold_results = []

    print(f"\nğŸ“Š K-fold äº¤å‰é©—è­‰ï¼ˆ{folds} foldsï¼‰")
    for fold, (train_idx, test_idx) in enumerate(kf.split(X), start=1):
        model = create_regressor()
        model.fit(X[train_idx], y[train_idx])
        y_pred = model.predict(X[test_idx])
        rmse = float(np.sqrt(mean_squared_error(y[test_idx], y_pred)))
        r2 = float(r2_score(y[test_idx], y_pred))
        fold_results.append({'fold': fold, 'rmse': rmse, 'r2': r2})
        print(f"  Fold {fold}: RMSE={rmse:.4f}, RÂ²={r2:.4f}")

    avg_rmse = float(np.mean([item['rmse'] for item in fold_results]))
    avg_r2 = float(np.mean([item['r2'] for item in fold_results]))
    print(f"  å¹³å‡: RMSE={avg_rmse:.4f}, RÂ²={avg_r2:.4f}")

    return {
        'folds': fold_results,
        'average_rmse': avg_rmse,
        'average_r2': avg_r2
    }


def generate_health_report(records: List[Dict], X: np.ndarray, y: np.ndarray) -> Dict:
    keyword_counter = Counter()
    locale_counter = Counter()
    serp_rank_counter = Counter()
    feature_missing = Counter({name: 0 for name in FEATURE_NAMES})
    timestamps: List[datetime] = []

    for record in records:
        keyword = (record.get('keyword') or '').strip()
        if keyword:
            keyword_counter[keyword.lower()] += 1

        locale = (record.get('locale') or '').strip() or 'unknown'
        locale_counter[locale] += 1

        rank = record.get('serp_rank')
        if isinstance(rank, (int, float)):
            serp_rank_counter[int(rank)] += 1

        features = record.get('features') or {}
        for name in FEATURE_NAMES:
            if not feature_present(features, name):
                feature_missing[name] += 1

        ts = extract_timestamp(record)
        if ts:
            timestamps.append(ts)

    total_records = max(len(records), 1)
    feature_coverage = {
        name: {
            'missingCount': missing,
            'coverageRatio': 1 - (missing / total_records)
        }
        for name, missing in feature_missing.items()
    }

    keyword_top = keyword_counter.most_common(10)
    serp_distribution = {str(rank): count for rank, count in sorted(serp_rank_counter.items())}
    locale_distribution = dict(locale_counter)

    ts_min = min(timestamps).isoformat() if timestamps else None
    ts_max = max(timestamps).isoformat() if timestamps else None

    return {
        'dataset': {
            'records': len(records),
            'features': X.shape[1],
            'dateRange': {'start': ts_min, 'end': ts_max}
        },
        'targetStats': {
            'mean': float(np.mean(y)),
            'std': float(np.std(y)),
            'min': float(np.min(y)),
            'max': float(np.max(y))
        },
        'keywordStats': {
            'uniqueKeywords': len(keyword_counter),
            'topKeywords': [{'keyword': k, 'count': c} for k, c in keyword_top]
        },
        'localeStats': locale_distribution,
        'serpRankStats': serp_distribution,
        'featureCoverage': feature_coverage
    }


def extract_model_coefficients(model: XGBRegressor) -> Dict:
    """
    å¾è¨“ç·´å¥½çš„æ¨¡å‹æå–ä¿‚æ•¸èˆ‡é…ç½®
    
    Args:
        model: è¨“ç·´å¥½çš„ XGBoost æ¨¡å‹
    
    Returns:
        æ¨¡å‹é…ç½®å­—å…¸
    """
    # æå–ç‰¹å¾µé‡è¦æ€§
    feature_importance = model.get_booster().get_score(importance_type='weight')
    
    # æå–æ¨¹çµæ§‹ï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰
    booster = model.get_booster()
    
    config = {
        'model_type': 'xgboost',
        'n_estimators': model.n_estimators,
        'max_depth': model.max_depth,
        'learning_rate': float(model.learning_rate),
        'subsample': model.subsample,
        'colsample_bytree': model.colsample_bytree,
        'feature_importance': feature_importance,
        'feature_names': [
            'hcuYesRatio', 'hcuPartialRatio', 'hcuNoRatio',
            'titleIntentMatch', 'firstParagraphAnswerQuality', 'qaFormatScore',
            'wordCountNorm', 'topicCohesion', 'semanticParagraphFocus',
            'referenceKeywordNorm', 'actionableScoreNorm',
            'avgSentenceLengthNorm', 'longParagraphPenalty',
            'listCount', 'tableCount',
            'authorInfoPresent', 'brandEntityClarity', 'externalCitationCount',
            'socialMediaLinksPresent', 'reviewRatingPresent',
            'semanticNaturalness', 'paragraphExtractability', 'richSnippetFormat',
            'citabilityTrustScore', 'multimediaSupport'
        ]
    }
    
    return config


def save_model(model: XGBRegressor, output_path: str) -> str:
    """
    ä¿å­˜è¨“ç·´å¥½çš„æ¨¡å‹
    
    Args:
        model: è¨“ç·´å¥½çš„æ¨¡å‹
        output_path: è¼¸å‡ºè·¯å¾‘
    
    Returns:
        ä¿å­˜çš„æª”æ¡ˆè·¯å¾‘
    """
    output_dir = Path(output_path).parent
    output_dir.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'wb') as f:
        pickle.dump(model, f)
    
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜: {output_path}")
    return output_path


def generate_model_config(
    model: XGBRegressor,
    metrics: Dict,
    records_count: int,
    output_path: str,
    evaluation_summary: Optional[Dict] = None
) -> Dict:
    """
    ç”Ÿæˆæ¨¡å‹é…ç½®æª”æ¡ˆï¼ˆç”¨æ–¼ Worker éƒ¨ç½²ï¼‰
    
    Args:
        model: è¨“ç·´å¥½çš„æ¨¡å‹
        metrics: è©•ä¼°æŒ‡æ¨™
        records_count: è¨“ç·´è¨˜éŒ„æ•¸
        output_path: è¼¸å‡ºè·¯å¾‘
        evaluation_summary: è©•ä¼°æ‘˜è¦
    
    Returns:
        æ¨¡å‹é…ç½®å­—å…¸
    """
    coefficients = extract_model_coefficients(model)
    
    config = {
        'version': f"2025-11-11-ml-v{datetime.now().strftime('%H%M%S')}",
        'createdAt': datetime.now().isoformat(),
        'description': 'XGBoost scoring model trained on SERP data',
        'trainingMetrics': metrics,
        'trainingRecords': records_count,
        'modelConfig': coefficients,
        'deployment': {
            'type': 'xgboost',
            'format': 'json',
            'compatibility': 'scoring-model.js v2.0+'
        }
    }
    
    if evaluation_summary:
        config['evaluationSummary'] = evaluation_summary
    
    output_dir = Path(output_path).parent
    output_dir.mkdir(parents=True, exist_ok=True)
    
    write_json(output_path, config)
    print(f"âœ… æ¨¡å‹é…ç½®å·²ç”Ÿæˆ: {output_path}")
    return config


def generate_deployment_script(
    model_config_path: str,
    output_path: str
) -> str:
    """
    ç”Ÿæˆéƒ¨ç½²è…³æœ¬ï¼ˆç”¨æ–¼æ›´æ–° scoring-model.jsï¼‰
    
    Args:
        model_config_path: æ¨¡å‹é…ç½®æª”æ¡ˆè·¯å¾‘
        output_path: è¼¸å‡ºè…³æœ¬è·¯å¾‘
    
    Returns:
        éƒ¨ç½²è…³æœ¬è·¯å¾‘
    """
    with open(model_config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)
    
    script = f"""#!/bin/bash
# è‡ªå‹•éƒ¨ç½²è…³æœ¬ - æ›´æ–° scoring-model.js
# ç”Ÿæˆæ™‚é–“: {datetime.now().isoformat()}

echo "ğŸ“¦ é–‹å§‹éƒ¨ç½²æ–°æ¨¡å‹..."

# 1. å‚™ä»½ç¾æœ‰æ¨¡å‹
cp functions/api/scoring-model.js functions/api/scoring-model.js.backup

# 2. æ›´æ–°æ¨¡å‹é…ç½®
# æ­¤è™•æ‡‰å°‡ {model_config_path} çš„å…§å®¹åˆä½µåˆ° scoring-model.js

# 3. é©—è­‰èªæ³•
node -c functions/api/scoring-model.js

# 4. éƒ¨ç½²åˆ° Cloudflare
wrangler deploy

# 5. é©—è­‰éƒ¨ç½²
curl -X GET "https://api.example.com/api/health"

echo "âœ… éƒ¨ç½²å®Œæˆ"
echo "æ¨¡å‹ç‰ˆæœ¬: {config['version']}"
echo "è¨“ç·´è¨˜éŒ„: {config['trainingRecords']}"
echo "æ¸¬è©¦ RÂ²: {config['trainingMetrics']['test_r2']:.4f}"
"""
    
    output_dir = Path(output_path).parent
    output_dir.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(script)
    
    # è¨­å®šåŸ·è¡Œæ¬Šé™
    os.chmod(output_path, 0o755)
    
    print(f"âœ… éƒ¨ç½²è…³æœ¬å·²ç”Ÿæˆ: {output_path}")
    return output_path


def update_worker_model(worker_model_path: str, model_config: Dict) -> None:
    """åœ¨ scoring-model.js ä¸­æ›´æ–°è‡ªå‹•ç”¢ç”Ÿçš„æ¨¡å‹å€å¡Šã€‚"""
    js_text = Path(worker_model_path).read_text(encoding='utf-8')
    serialized = json.dumps(model_config, ensure_ascii=False, indent=2)
    replacement = (
        "/* AUTO-GENERATED:MODEL_CONFIG_START */\n"
        f"export const AUTO_GENERATED_MODEL_CONFIG = {serialized};\n"
        "/* AUTO-GENERATED:MODEL_CONFIG_END */"
    )

    pattern = re.compile(
        r"/\* AUTO-GENERATED:MODEL_CONFIG_START \*/[\s\S]*?/\* AUTO-GENERATED:MODEL_CONFIG_END \*/",
        re.MULTILINE
    )

    if not pattern.search(js_text):
        raise RuntimeError('æ‰¾ä¸åˆ° scoring-model.js çš„ AUTO-GENERATED å€å¡Š')

    new_text = pattern.sub(replacement, js_text, count=1)
    Path(worker_model_path).write_text(new_text, encoding='utf-8')
    print(f"âœ… å·²æ›´æ–° {worker_model_path} çš„æ¨¡å‹é…ç½®")


def send_webhook(webhook_url: str, data: Dict) -> None:
    if not webhook_url:
        return
    try:
        payload = json.dumps(data).encode('utf-8')
        req = urllib.request.Request(
            webhook_url,
            data=payload,
            headers={'Content-Type': 'application/json'}
        )
        with urllib.request.urlopen(req, timeout=15) as resp:
            status = resp.getcode()
            if 200 <= status < 300:
                print(f"âœ… å¥åº·å ±è¡¨å·²é€é” Webhook ({status})")
            else:
                print(f"âš ï¸ Webhook å›æ‡‰ç‹€æ…‹ç¢¼ {status}")
    except urllib.error.URLError as error:
        print(f"âš ï¸ Webhook ç™¼é€å¤±æ•—: {error}")


def upload_health_report_to_r2(target: str, data: Dict) -> None:
    if ':' not in target:
        print('âš ï¸ health-report-r2 åƒæ•¸æ ¼å¼æ‡‰ç‚º bucket:key')
        return
    bucket, key = target.split(':', 1)
    with tempfile.NamedTemporaryFile(delete=False, suffix='.json', mode='w', encoding='utf-8') as tmp:
        json.dump(data, tmp, ensure_ascii=False, indent=2)
        tmp_path = tmp.name
    try:
        cmd = ['wrangler', 'r2', 'object', 'put', f'{bucket}/{key}', '--file', tmp_path]
        subprocess.run(cmd, check=True)
        print(f"âœ… å¥åº·å ±è¡¨å·²ä¸Šå‚³è‡³ R2 {bucket}/{key}")
    except (subprocess.CalledProcessError, FileNotFoundError) as error:
        print(f"âš ï¸ ç„¡æ³•ä¸Šå‚³è‡³ R2: {error}")
    finally:
        try:
            os.unlink(tmp_path)
        except OSError:
            pass


def write_json(output_path: str, data: Dict) -> None:
    """
    å¯«å…¥ JSON æª”æ¡ˆ
    
    Args:
        output_path: è¼¸å‡ºè·¯å¾‘
        data: å¯«å…¥è³‡æ–™
    """
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def main():
    """ä¸»ç¨‹å¼"""
    parser = argparse.ArgumentParser(
        description='éäº’å‹•å¼æ¨¡å‹è¨“ç·´å·¥å…·'
    )
    parser.add_argument(
        '--data-dir',
        required=True,
        help='è¨“ç·´è³‡æ–™ç›®éŒ„'
    )
    parser.add_argument(
        '--output-dir',
        default='./ml/models',
        help='è¼¸å‡ºç›®éŒ„'
    )
    parser.add_argument(
        '--model-name',
        default='xgboost_model',
        help='æ¨¡å‹åç¨±'
    )
    parser.add_argument(
        '--test-size',
        type=float,
        default=0.2,
        help='æ¸¬è©¦é›†æ¯”ä¾‹'
    )
    parser.add_argument(
        '--time-split',
        action='store_true',
        help='å•Ÿç”¨æ™‚é–“æ’åºåˆ‡åˆ†ï¼ˆä»¥ timestamp åˆ†å‰²è¨“ç·´/æ¸¬è©¦ï¼‰'
    )
    parser.add_argument(
        '--kfold',
        type=int,
        default=0,
        help='è‹¥ >1ï¼ŒåŸ·è¡Œ K-fold äº¤å‰é©—è­‰ä¸¦è¼¸å‡ºçµ±è¨ˆ'
    )
    parser.add_argument(
        '--deploy',
        action='store_true',
        help='ç”Ÿæˆéƒ¨ç½²è…³æœ¬'
    )
    parser.add_argument(
        '--worker-model-path',
        default='functions/api/scoring-model.js',
        help='Worker scoring-model.js è·¯å¾‘ï¼ˆè‡ªå‹•å¯«å…¥æ¨¡å‹ configï¼‰'
    )
    parser.add_argument(
        '--health-report',
        help='è¼¸å‡ºè³‡æ–™å¥åº·å ±è¡¨ JSON çš„è·¯å¾‘'
    )
    parser.add_argument(
        '--health-report-webhook',
        help='å°‡å¥åº·å ±è¡¨ POST è‡³æŒ‡å®š URL'
    )
    parser.add_argument(
        '--health-report-r2',
        help='è¼¸å‡ºå¥åº·å ±è¡¨è‡³æœ¬åœ°æª”æ¡ˆä»¥ä¾›å¾ŒçºŒä¸Šå‚³ï¼ˆæ ¼å¼ï¼šbucket:path)'
    )
    
    args = parser.parse_args()
    
    try:
        # 1. è¼‰å…¥è¨“ç·´è³‡æ–™
        print("ğŸ“¥ è¼‰å…¥è¨“ç·´è³‡æ–™...")
        records, X, y = load_training_data(args.data_dir)

        # 1.1 ç”¢å‡ºè³‡æ–™å¥åº·å ±è¡¨
        health_report = generate_health_report(records, X, y)
        if args.health_report:
            write_json(args.health_report, health_report)
            print(f"  ğŸ©º å·²è¼¸å‡ºå¥åº·å ±è¡¨: {args.health_report}")
        if args.health_report_webhook:
            send_webhook(args.health_report_webhook, health_report)
        if args.health_report_r2:
            write_json(args.health_report_r2, health_report)
            print(f"  ğŸª£ å·²è¼¸å‡ºå¥åº·å ±è¡¨è‡³ R2 æº–å‚™æª”æ¡ˆ: {args.health_report_r2}")
        
        # 2. è¨“ç·´æ¨¡å‹
        print("\nğŸ¤– è¨“ç·´æ¨¡å‹...")
        model, metrics = train_model(
            X,
            y,
            test_size=args.test_size,
            records=records,
            time_split=args.time_split
        )

        kfold_report = None
        if args.kfold and args.kfold > 1:
            kfold_report = run_kfold_evaluation(X, y, args.kfold)
        
        # 3. ä¿å­˜æ¨¡å‹
        print("\nğŸ’¾ ä¿å­˜æ¨¡å‹...")
        output_dir = Path(args.output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        model_path = output_dir / f"{args.model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl"
        save_model(model, str(model_path))
        
        # 4. ç”Ÿæˆæ¨¡å‹é…ç½®
        print("\nğŸ“‹ ç”Ÿæˆæ¨¡å‹é…ç½®...")
        config_path = output_dir / f"{args.model_name}_config_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        model_config = generate_model_config(
            model,
            metrics,
            len(records),
            str(config_path),
            evaluation_summary={
                'time_split': args.time_split,
                'test_size': args.test_size,
                'kfold': kfold_report
            }
        )

        update_worker_model(args.worker_model_path, model_config)
        
        # 5. ç”Ÿæˆéƒ¨ç½²è…³æœ¬ï¼ˆå¯é¸ï¼‰
        if args.deploy:
            print("\nğŸš€ ç”Ÿæˆéƒ¨ç½²è…³æœ¬...")
            deploy_script_path = output_dir / f"deploy_{datetime.now().strftime('%Y%m%d_%H%M%S')}.sh"
            generate_deployment_script(str(config_path), str(deploy_script_path))
        
        # 6. ç”Ÿæˆæ‘˜è¦
        print("\nğŸ“Š è¨“ç·´æ‘˜è¦")
        print(f"  è¨“ç·´è¨˜éŒ„: {len(records)}")
        print(f"  ç‰¹å¾µæ•¸: {X.shape[1]}")
        print(f"  æ¸¬è©¦ RMSE: {metrics['test_rmse']:.4f}")
        print(f"  æ¸¬è©¦ RÂ²: {metrics['test_r2']:.4f}")
        print(f"  æ¨¡å‹è·¯å¾‘: {model_path}")
        print(f"  é…ç½®è·¯å¾‘: {config_path}")
        
        print("\nâœ… è¨“ç·´å®Œæˆ")
        
    except Exception as exc:
        print(f"\nâŒ ç™¼ç”ŸéŒ¯èª¤: {exc}")
        sys.exit(1)


if __name__ == '__main__':
    main()
