#!/usr/bin/env python3
"""
æ¨¡å‹è¨“ç·´ CLI å·¥å…·
æ”¯æ´éäº’å‹•å¼è¨“ç·´ã€æ¨¡å‹è½‰æª”ã€è‡ªå‹•éƒ¨ç½²
"""

import os
import sys
import json
import argparse
import pickle
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import numpy as np
from sklearn.ensemble import XGBRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# å‡è¨­å·²æœ‰çš„è¨“ç·´è³‡æ–™è¼‰å…¥å‡½æ•¸
def load_training_data(data_dir: str) -> Tuple[List[Dict], np.ndarray, np.ndarray]:
    """
    å¾æŒ‡å®šç›®éŒ„è¼‰å…¥è¨“ç·´è³‡æ–™
    
    Args:
        data_dir: è¨“ç·´è³‡æ–™ç›®éŒ„ï¼ˆåŒ…å« CSV/JSON æª”æ¡ˆï¼‰
    
    Returns:
        (records, X, y) - è¨˜éŒ„ã€ç‰¹å¾µã€æ¨™ç±¤
    """
    data_path = Path(data_dir)
    
    if not data_path.exists():
        raise FileNotFoundError(f"è³‡æ–™ç›®éŒ„ä¸å­˜åœ¨: {data_dir}")
    
    records = []
    
    # è¼‰å…¥ JSON æª”æ¡ˆ
    json_files = list(data_path.glob('*.json'))
    for json_file in json_files:
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if isinstance(data, list):
                    records.extend(data)
                elif isinstance(data, dict) and 'records' in data:
                    records.extend(data['records'])
        except Exception as e:
            print(f"âš ï¸ è¼‰å…¥ {json_file} å¤±æ•—: {e}")
    
    if not records:
        raise ValueError(f"æœªæ‰¾åˆ°è¨“ç·´è³‡æ–™: {data_dir}")
    
    print(f"âœ… å·²è¼‰å…¥ {len(records)} ç­†è¨“ç·´è¨˜éŒ„")
    
    # æå–ç‰¹å¾µèˆ‡æ¨™ç±¤
    X, y = extract_features_and_labels(records)
    
    return records, X, y


def extract_features_and_labels(records: List[Dict]) -> Tuple[np.ndarray, np.ndarray]:
    """
    å¾è¨˜éŒ„ä¸­æå–ç‰¹å¾µèˆ‡æ¨™ç±¤
    
    Args:
        records: è¨“ç·´è¨˜éŒ„æ¸…å–®
    
    Returns:
        (X, y) - ç‰¹å¾µçŸ©é™£èˆ‡æ¨™ç±¤å‘é‡
    """
    # å®šç¾©ç‰¹å¾µæ¬„ä½
    feature_fields = [
        'hcuYesRatio', 'hcuPartialRatio', 'hcuNoRatio',
        'titleIntentMatch', 'firstParagraphAnswerQuality', 'qaFormatScore',
        'wordCountNorm', 'topicCohesion', 'semanticParagraphFocus',
        'referenceKeywordNorm', 'actionableScoreNorm',
        'avgSentenceLengthNorm', 'longParagraphPenalty',
        'listCount', 'tableCount',
        'authorInfoPresent', 'brandEntityClarity', 'externalCitationCount',
        'socialMediaLinksPresent', 'reviewRatingPresent',
        'semanticNaturalness', 'paragraphExtractability', 'richSnippetFormat',
        'citabilityTrustScore', 'multimediaSupport'
    ]
    
    X_list = []
    y_list = []
    
    for record in records:
        # æå–ç‰¹å¾µ
        features = record.get('features', {})
        feature_vector = []
        
        for field in feature_fields:
            value = features.get(field, 0)
            # ç¢ºä¿å€¼åœ¨ 0-1 ç¯„åœå…§
            if isinstance(value, (int, float)):
                feature_vector.append(max(0, min(1, float(value))))
            else:
                feature_vector.append(0)
        
        # æå–æ¨™ç±¤ï¼ˆç›®æ¨™åˆ†æ•¸ï¼‰
        target_score = record.get('target_score')
        if target_score is not None:
            X_list.append(feature_vector)
            y_list.append(float(target_score))
    
    if not X_list:
        raise ValueError("æœªæ‰¾åˆ°æœ‰æ•ˆçš„è¨“ç·´è³‡æ–™")
    
    X = np.array(X_list)
    y = np.array(y_list)
    
    print(f"âœ… ç‰¹å¾µæå–å®Œæˆ: {X.shape[0]} ç­†è¨˜éŒ„, {X.shape[1]} å€‹ç‰¹å¾µ")
    
    return X, y


def train_model(X: np.ndarray, y: np.ndarray, test_size: float = 0.2) -> Tuple[XGBRegressor, Dict]:
    """
    è¨“ç·´ XGBoost æ¨¡å‹
    
    Args:
        X: ç‰¹å¾µçŸ©é™£
        y: æ¨™ç±¤å‘é‡
        test_size: æ¸¬è©¦é›†æ¯”ä¾‹
    
    Returns:
        (model, metrics) - è¨“ç·´å¥½çš„æ¨¡å‹èˆ‡è©•ä¼°æŒ‡æ¨™
    """
    print("ğŸ¤– é–‹å§‹è¨“ç·´æ¨¡å‹...")
    
    # åˆ†å‰²è¨“ç·´/æ¸¬è©¦é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42
    )
    
    print(f"  è¨“ç·´é›†: {X_train.shape[0]} ç­†")
    print(f"  æ¸¬è©¦é›†: {X_test.shape[0]} ç­†")
    
    # è¨“ç·´æ¨¡å‹
    model = XGBRegressor(
        n_estimators=100,
        max_depth=6,
        learning_rate=0.1,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        n_jobs=-1
    )
    
    model.fit(X_train, y_train)
    
    # è©•ä¼°æ¨¡å‹
    y_pred_train = model.predict(X_train)
    y_pred_test = model.predict(X_test)
    
    metrics = {
        'train_mse': float(mean_squared_error(y_train, y_pred_train)),
        'test_mse': float(mean_squared_error(y_test, y_pred_test)),
        'train_rmse': float(np.sqrt(mean_squared_error(y_train, y_pred_train))),
        'test_rmse': float(np.sqrt(mean_squared_error(y_test, y_pred_test))),
        'train_mae': float(mean_absolute_error(y_train, y_pred_train)),
        'test_mae': float(mean_absolute_error(y_test, y_pred_test)),
        'train_r2': float(r2_score(y_train, y_pred_train)),
        'test_r2': float(r2_score(y_test, y_pred_test))
    }
    
    print(f"âœ… æ¨¡å‹è¨“ç·´å®Œæˆ")
    print(f"  è¨“ç·´ RMSE: {metrics['train_rmse']:.4f}")
    print(f"  æ¸¬è©¦ RMSE: {metrics['test_rmse']:.4f}")
    print(f"  è¨“ç·´ RÂ²: {metrics['train_r2']:.4f}")
    print(f"  æ¸¬è©¦ RÂ²: {metrics['test_r2']:.4f}")
    
    return model, metrics


def extract_model_coefficients(model: XGBRegressor) -> Dict:
    """
    å¾è¨“ç·´å¥½çš„æ¨¡å‹æå–ä¿‚æ•¸èˆ‡é…ç½®
    
    Args:
        model: è¨“ç·´å¥½çš„ XGBoost æ¨¡å‹
    
    Returns:
        æ¨¡å‹é…ç½®å­—å…¸
    """
    # æå–ç‰¹å¾µé‡è¦æ€§
    feature_importance = model.get_booster().get_score(importance_type='weight')
    
    # æå–æ¨¹çµæ§‹ï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰
    booster = model.get_booster()
    
    config = {
        'model_type': 'xgboost',
        'n_estimators': model.n_estimators,
        'max_depth': model.max_depth,
        'learning_rate': float(model.learning_rate),
        'subsample': model.subsample,
        'colsample_bytree': model.colsample_bytree,
        'feature_importance': feature_importance,
        'feature_names': [
            'hcuYesRatio', 'hcuPartialRatio', 'hcuNoRatio',
            'titleIntentMatch', 'firstParagraphAnswerQuality', 'qaFormatScore',
            'wordCountNorm', 'topicCohesion', 'semanticParagraphFocus',
            'referenceKeywordNorm', 'actionableScoreNorm',
            'avgSentenceLengthNorm', 'longParagraphPenalty',
            'listCount', 'tableCount',
            'authorInfoPresent', 'brandEntityClarity', 'externalCitationCount',
            'socialMediaLinksPresent', 'reviewRatingPresent',
            'semanticNaturalness', 'paragraphExtractability', 'richSnippetFormat',
            'citabilityTrustScore', 'multimediaSupport'
        ]
    }
    
    return config


def save_model(model: XGBRegressor, output_path: str) -> str:
    """
    ä¿å­˜è¨“ç·´å¥½çš„æ¨¡å‹
    
    Args:
        model: è¨“ç·´å¥½çš„æ¨¡å‹
        output_path: è¼¸å‡ºè·¯å¾‘
    
    Returns:
        ä¿å­˜çš„æª”æ¡ˆè·¯å¾‘
    """
    output_dir = Path(output_path).parent
    output_dir.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'wb') as f:
        pickle.dump(model, f)
    
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜: {output_path}")
    return output_path


def generate_model_config(
    model: XGBRegressor,
    metrics: Dict,
    records_count: int,
    output_path: str
) -> str:
    """
    ç”Ÿæˆæ¨¡å‹é…ç½®æª”æ¡ˆï¼ˆç”¨æ–¼ Worker éƒ¨ç½²ï¼‰
    
    Args:
        model: è¨“ç·´å¥½çš„æ¨¡å‹
        metrics: è©•ä¼°æŒ‡æ¨™
        records_count: è¨“ç·´è¨˜éŒ„æ•¸
        output_path: è¼¸å‡ºè·¯å¾‘
    
    Returns:
        é…ç½®æª”æ¡ˆè·¯å¾‘
    """
    coefficients = extract_model_coefficients(model)
    
    config = {
        'version': f"2025-11-11-ml-v{datetime.now().strftime('%H%M%S')}",
        'createdAt': datetime.now().isoformat(),
        'description': 'XGBoost scoring model trained on SERP data',
        'trainingMetrics': metrics,
        'trainingRecords': records_count,
        'modelConfig': coefficients,
        'deployment': {
            'type': 'xgboost',
            'format': 'json',
            'compatibility': 'scoring-model.js v2.0+'
        }
    }
    
    output_dir = Path(output_path).parent
    output_dir.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… æ¨¡å‹é…ç½®å·²ç”Ÿæˆ: {output_path}")
    return output_path


def generate_deployment_script(
    model_config_path: str,
    output_path: str
) -> str:
    """
    ç”Ÿæˆéƒ¨ç½²è…³æœ¬ï¼ˆç”¨æ–¼æ›´æ–° scoring-model.jsï¼‰
    
    Args:
        model_config_path: æ¨¡å‹é…ç½®æª”æ¡ˆè·¯å¾‘
        output_path: è¼¸å‡ºè…³æœ¬è·¯å¾‘
    
    Returns:
        éƒ¨ç½²è…³æœ¬è·¯å¾‘
    """
    with open(model_config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)
    
    script = f"""#!/bin/bash
# è‡ªå‹•éƒ¨ç½²è…³æœ¬ - æ›´æ–° scoring-model.js
# ç”Ÿæˆæ™‚é–“: {datetime.now().isoformat()}

echo "ğŸ“¦ é–‹å§‹éƒ¨ç½²æ–°æ¨¡å‹..."

# 1. å‚™ä»½ç¾æœ‰æ¨¡å‹
cp functions/api/scoring-model.js functions/api/scoring-model.js.backup

# 2. æ›´æ–°æ¨¡å‹é…ç½®
# æ­¤è™•æ‡‰å°‡ {model_config_path} çš„å…§å®¹åˆä½µåˆ° scoring-model.js

# 3. é©—è­‰èªæ³•
node -c functions/api/scoring-model.js

# 4. éƒ¨ç½²åˆ° Cloudflare
wrangler deploy

# 5. é©—è­‰éƒ¨ç½²
curl -X GET "https://api.example.com/api/health"

echo "âœ… éƒ¨ç½²å®Œæˆ"
echo "æ¨¡å‹ç‰ˆæœ¬: {config['version']}"
echo "è¨“ç·´è¨˜éŒ„: {config['trainingRecords']}"
echo "æ¸¬è©¦ RÂ²: {config['trainingMetrics']['test_r2']:.4f}"
"""
    
    output_dir = Path(output_path).parent
    output_dir.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(script)
    
    # è¨­å®šåŸ·è¡Œæ¬Šé™
    os.chmod(output_path, 0o755)
    
    print(f"âœ… éƒ¨ç½²è…³æœ¬å·²ç”Ÿæˆ: {output_path}")
    return output_path


def main():
    """ä¸»ç¨‹å¼"""
    parser = argparse.ArgumentParser(
        description='éäº’å‹•å¼æ¨¡å‹è¨“ç·´å·¥å…·'
    )
    parser.add_argument(
        '--data-dir',
        required=True,
        help='è¨“ç·´è³‡æ–™ç›®éŒ„'
    )
    parser.add_argument(
        '--output-dir',
        default='./ml/models',
        help='è¼¸å‡ºç›®éŒ„'
    )
    parser.add_argument(
        '--model-name',
        default='xgboost_model',
        help='æ¨¡å‹åç¨±'
    )
    parser.add_argument(
        '--test-size',
        type=float,
        default=0.2,
        help='æ¸¬è©¦é›†æ¯”ä¾‹'
    )
    parser.add_argument(
        '--deploy',
        action='store_true',
        help='ç”Ÿæˆéƒ¨ç½²è…³æœ¬'
    )
    
    args = parser.parse_args()
    
    try:
        # 1. è¼‰å…¥è¨“ç·´è³‡æ–™
        print("ğŸ“¥ è¼‰å…¥è¨“ç·´è³‡æ–™...")
        records, X, y = load_training_data(args.data_dir)
        
        # 2. è¨“ç·´æ¨¡å‹
        print("\nğŸ¤– è¨“ç·´æ¨¡å‹...")
        model, metrics = train_model(X, y, test_size=args.test_size)
        
        # 3. ä¿å­˜æ¨¡å‹
        print("\nğŸ’¾ ä¿å­˜æ¨¡å‹...")
        output_dir = Path(args.output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        model_path = output_dir / f"{args.model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl"
        save_model(model, str(model_path))
        
        # 4. ç”Ÿæˆæ¨¡å‹é…ç½®
        print("\nğŸ“‹ ç”Ÿæˆæ¨¡å‹é…ç½®...")
        config_path = output_dir / f"{args.model_name}_config_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        generate_model_config(model, metrics, len(records), str(config_path))
        
        # 5. ç”Ÿæˆéƒ¨ç½²è…³æœ¬ï¼ˆå¯é¸ï¼‰
        if args.deploy:
            print("\nğŸš€ ç”Ÿæˆéƒ¨ç½²è…³æœ¬...")
            deploy_script_path = output_dir / f"deploy_{datetime.now().strftime('%Y%m%d_%H%M%S')}.sh"
            generate_deployment_script(str(config_path), str(deploy_script_path))
        
        # 6. ç”Ÿæˆæ‘˜è¦
        print("\nğŸ“Š è¨“ç·´æ‘˜è¦")
        print(f"  è¨“ç·´è¨˜éŒ„: {len(records)}")
        print(f"  ç‰¹å¾µæ•¸: {X.shape[1]}")
        print(f"  æ¸¬è©¦ RMSE: {metrics['test_rmse']:.4f}")
        print(f"  æ¸¬è©¦ RÂ²: {metrics['test_r2']:.4f}")
        print(f"  æ¨¡å‹è·¯å¾‘: {model_path}")
        print(f"  é…ç½®è·¯å¾‘: {config_path}")
        
        print("\nâœ… è¨“ç·´å®Œæˆ")
        
    except Exception as e:
        print(f"\nâŒ è¨“ç·´å¤±æ•—: {e}")
        sys.exit(1)


if __name__ == '__main__':
    main()
